2024. 07. 17에 진행한 세미나 기록
- (1) hongong/FAQ.md 1~13번에 대한 해설
- (2) 블로그 자료 (LR, SVM, DT, RF)에 대한 해설
- (3) RNN과 LSTM에 대한 개괄

(1)
- 일반적 프로그래밍과 머신러닝 프로그래밍과의 차이 : 규칙 (rule)의 수동 입력? 자동 입력?
- 지도, 비지도 학습의 차이 : label (target, 정답)의 유무
    - 비지도 학습의 연구가 늘어나는 이유 : 모든 데이터에 label을 붙이는 과정이 까다로움 
    - 따라서 label을 자동으로 붙이는 기술도 develop
- 차원의 저주 : 차원이 너무 많아질 수록 overfitting될 수도 있으며, 이해가 어렵다
    - 따라서 feature selection이나 feature engineering과 같은 기술로 이를 해결
    - 중요도가 높은 feature나 상관관계가 높은 feature만 extraction하여 fit
- ridge vs lasso : 규제 (regularization) -> overfitting 방지
- 절편과 기울기가 딥러닝에서 사용되는 것은 퍼셉트론 각각의 weight와 bias

(2)
[LR]
- https://ratsgo.github.io/machine%20learning/2017/04/02/logistic/
- https://m.blog.naver.com/winddori2002/221706766540
[SVM]
- https://m.blog.naver.com/winddori2002/221662413641
[DT]
- https://lucy-the-marketer.kr/ko/growth/decision-tree-and-impurity/
[RF]
- https://velog.io/@dlskawns/Machine-Learning-Random-Forest-정리-구성원리-파악-및-모델-작성

(3) 
- back progation에서 역전파 시 기울기 소실이 발생 (미분이 계속 진행되다가 0이 됨) 
- 최근 사실을 까먹음 (short term loss) -> LSTM 탄생 (long & short term) -> GPU 사용 (속도를 계산한 것 : GRU)