{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "dbd37246",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T13:01:00.624071Z",
     "start_time": "2025-04-26T13:01:00.605309Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from io import BytesIO\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "3a2a6263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T13:01:06.235740Z",
     "start_time": "2025-04-26T13:01:00.628900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded inertial signals data with shape: (7352, 128, 9) (train), (2947, 128, 9) (test)\n",
      "Training data shape: (7352, 128, 9)\n",
      "Training labels shape: (7352, 6)\n",
      "Test data shape: (2947, 128, 9)\n",
      "Test labels shape: (2947, 6)\n",
      "\n",
      "Activity labels:\n",
      "   class_index          class_name\n",
      "0            1             WALKING\n",
      "1            2    WALKING_UPSTAIRS\n",
      "2            3  WALKING_DOWNSTAIRS\n",
      "3            4             SITTING\n",
      "4            5            STANDING\n",
      "5            6              LAYING\n"
     ]
    }
   ],
   "source": [
    "def load_uci_har_data(data_path):\n",
    "    # Load activity labels\n",
    "    activity_labels = pd.read_csv(os.path.join(data_path, 'activity_labels.txt'), \n",
    "                                 delim_whitespace=True, header=None, \n",
    "                                 names=['class_index', 'class_name'])\n",
    "    \n",
    "    # Load labels\n",
    "    y_train = pd.read_csv(os.path.join(data_path, 'train/y_train.txt'), header=None)\n",
    "    y_test = pd.read_csv(os.path.join(data_path, 'test/y_test.txt'), header=None)\n",
    "    \n",
    "    # Convert labels to numpy arrays\n",
    "    y_train = y_train.values.flatten().astype('int32') - 1  # Make labels zero-indexed\n",
    "    y_test = y_test.values.flatten().astype('int32') - 1  # Make labels zero-indexed\n",
    "    \n",
    "    # Load inertial signals data\n",
    "    train_signals_paths = [\n",
    "        os.path.join(data_path, 'train/Inertial Signals/body_acc_x_train.txt'),\n",
    "        os.path.join(data_path, 'train/Inertial Signals/body_acc_y_train.txt'),\n",
    "        os.path.join(data_path, 'train/Inertial Signals/body_acc_z_train.txt'),\n",
    "        os.path.join(data_path, 'train/Inertial Signals/body_gyro_x_train.txt'),\n",
    "        os.path.join(data_path, 'train/Inertial Signals/body_gyro_y_train.txt'),\n",
    "        os.path.join(data_path, 'train/Inertial Signals/body_gyro_z_train.txt'),\n",
    "        os.path.join(data_path, 'train/Inertial Signals/total_acc_x_train.txt'),\n",
    "        os.path.join(data_path, 'train/Inertial Signals/total_acc_y_train.txt'),\n",
    "        os.path.join(data_path, 'train/Inertial Signals/total_acc_z_train.txt')\n",
    "    ]\n",
    "    \n",
    "    test_signals_paths = [\n",
    "        os.path.join(data_path, 'test/Inertial Signals/body_acc_x_test.txt'),\n",
    "        os.path.join(data_path, 'test/Inertial Signals/body_acc_y_test.txt'),\n",
    "        os.path.join(data_path, 'test/Inertial Signals/body_acc_z_test.txt'),\n",
    "        os.path.join(data_path, 'test/Inertial Signals/body_gyro_x_test.txt'),\n",
    "        os.path.join(data_path, 'test/Inertial Signals/body_gyro_y_test.txt'),\n",
    "        os.path.join(data_path, 'test/Inertial Signals/body_gyro_z_test.txt'),\n",
    "        os.path.join(data_path, 'test/Inertial Signals/total_acc_x_test.txt'),\n",
    "        os.path.join(data_path, 'test/Inertial Signals/total_acc_y_test.txt'),\n",
    "        os.path.join(data_path, 'test/Inertial Signals/total_acc_z_test.txt')\n",
    "    ]\n",
    "    \n",
    "    # Load and stack training data\n",
    "    x_train_signals = []\n",
    "    for path in train_signals_paths:\n",
    "        signal = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
    "        x_train_signals.append(signal)\n",
    "    \n",
    "    # Load and stack test data\n",
    "    x_test_signals = []\n",
    "    for path in test_signals_paths:\n",
    "        signal = pd.read_csv(path, delim_whitespace=True, header=None).values\n",
    "        x_test_signals.append(signal)\n",
    "    \n",
    "    # Convert to numpy arrays with shape (n_samples, n_timestamps, n_features)\n",
    "    # Each signal has shape (n_samples, n_timestamps)\n",
    "    # We'll transpose to get shape (n_samples, n_timestamps, n_features)\n",
    "    x_train = np.transpose(np.array(x_train_signals), (1, 2, 0)).astype('float32')\n",
    "    x_test = np.transpose(np.array(x_test_signals), (1, 2, 0)).astype('float32')\n",
    "    \n",
    "    print(f\"Loaded inertial signals data with shape: {x_train.shape} (train), {x_test.shape} (test)\")\n",
    "    \n",
    "    # Normalize each feature independently\n",
    "    n_samples_train, n_timestamps, n_features = x_train.shape\n",
    "    n_samples_test = x_test.shape[0]\n",
    "    \n",
    "    # Reshape to normalize features independently\n",
    "    x_train_reshaped = x_train.reshape((n_samples_train * n_timestamps, n_features))\n",
    "    x_test_reshaped = x_test.reshape((n_samples_test * n_timestamps, n_features))\n",
    "    \n",
    "    # Normalize\n",
    "    scaler = StandardScaler()\n",
    "    x_train_reshaped = scaler.fit_transform(x_train_reshaped)\n",
    "    x_test_reshaped = scaler.transform(x_test_reshaped)\n",
    "    \n",
    "    # Reshape back to original shape\n",
    "    x_train = x_train_reshaped.reshape((n_samples_train, n_timestamps, n_features))\n",
    "    x_test = x_test_reshaped.reshape((n_samples_test, n_timestamps, n_features))\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes=6)\n",
    "    y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=6)\n",
    "    \n",
    "    return x_train, y_train, y_train_onehot, x_test, y_test, y_test_onehot, activity_labels\n",
    "\n",
    "data_path = \"../../data/UCI_HAR_Dataset/\"\n",
    "x_train, y_train, y_train_onehot, x_test, y_test, y_test_onehot, activity_labels = load_uci_har_data(data_path)\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train_onehot.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test_onehot.shape}\")\n",
    "print(\"\\nActivity labels:\")\n",
    "print(activity_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "749678f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T13:01:06.267972Z",
     "start_time": "2025-04-26T13:01:06.239622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data shape: (7352, 128, 9)\n",
      "Original test data shape: (2947, 128, 9)\n",
      "Final training data shape: (7352, 128, 9)\n",
      "Final training labels shape: (7352, 6)\n",
      "Final test data shape: (2947, 128, 9)\n",
      "Final test labels shape: (2947, 6)\n"
     ]
    }
   ],
   "source": [
    "def split_sequences(data, window_size=128, step=64):\n",
    "    \"\"\"\n",
    "    Split the data into sequences with overlapping windows\n",
    "    \n",
    "    Args:\n",
    "        data: Input data with shape (n_samples, n_features)\n",
    "        window_size: Number of time steps in each sequence\n",
    "        step: Step size between consecutive sequences\n",
    "        \n",
    "    Returns:\n",
    "        Sequences with shape (n_sequences, window_size, n_features)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = data.shape\n",
    "    sequences = []\n",
    "    \n",
    "    for i in range(0, n_samples - window_size + 1, step):\n",
    "        sequence = data[i:i + window_size]\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return np.array(sequences)\n",
    "\n",
    "# -----\n",
    "# Since the inertial signals data is already in the format of (samples, timestamps, features),\n",
    "# we don't need to use the split_sequences function for basic formatting, but we'll still use it\n",
    "# if needed for resizing or overlap\n",
    "\n",
    "# Check the current shape of the data\n",
    "print(f\"Original training data shape: {x_train.shape}\")\n",
    "print(f\"Original test data shape: {x_test.shape}\")\n",
    "\n",
    "# The data already has 128 timestamps per sample, so we can use it directly\n",
    "# If we need to adjust window size or create overlapping windows:\n",
    "window_size = 128  # This is already the size in the dataset\n",
    "step = 128  # Non-overlapping by default - set to smaller value for overlap\n",
    "\n",
    "# Check if we need to reshape the data\n",
    "if x_train.shape[1] != window_size or step != window_size:\n",
    "    print(\"Reshaping data to adjust window size or create overlapping samples...\")\n",
    "    # Flatten the data to 2D and then recreate sequences\n",
    "    x_train_flat = x_train.reshape(x_train.shape[0] * x_train.shape[1], x_train.shape[2])\n",
    "    x_test_flat = x_test.reshape(x_test.shape[0] * x_test.shape[1], x_test.shape[2])\n",
    "    \n",
    "    # Create new sequences\n",
    "    x_train_seq = split_sequences(x_train_flat, window_size=window_size, step=step)\n",
    "    x_test_seq = split_sequences(x_test_flat, window_size=window_size, step=step)\n",
    "    \n",
    "    # Create corresponding labels for sequences\n",
    "    # For each sequence, we'll use the mode of the labels\n",
    "    orig_samples_per_window = x_train.shape[1] / window_size\n",
    "    \n",
    "    y_train_seq = []\n",
    "    for i in range(0, len(x_train_flat) - window_size + 1, step):\n",
    "        # Map back to original sample indices\n",
    "        orig_sample_idx = int(i / x_train.shape[1])\n",
    "        mode_label = y_train[orig_sample_idx]\n",
    "        y_train_seq.append(mode_label)\n",
    "\n",
    "    y_test_seq = []\n",
    "    for i in range(0, len(x_test_flat) - window_size + 1, step):\n",
    "        # Map back to original sample indices\n",
    "        orig_sample_idx = int(i / x_test.shape[1])\n",
    "        mode_label = y_test[orig_sample_idx]\n",
    "        y_test_seq.append(mode_label)\n",
    "    \n",
    "    y_train_seq = np.array(y_train_seq)\n",
    "    y_test_seq = np.array(y_test_seq)\n",
    "else:\n",
    "    # If no reshaping is needed, use the data directly\n",
    "    x_train_seq = x_train\n",
    "    x_test_seq = x_test\n",
    "    y_train_seq = y_train\n",
    "    y_test_seq = y_test\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_seq_onehot = tf.keras.utils.to_categorical(y_train_seq, num_classes=6)\n",
    "y_test_seq_onehot = tf.keras.utils.to_categorical(y_test_seq, num_classes=6)\n",
    "\n",
    "print(f\"Final training data shape: {x_train_seq.shape}\")\n",
    "print(f\"Final training labels shape: {y_train_seq_onehot.shape}\")\n",
    "print(f\"Final test data shape: {x_test_seq.shape}\")\n",
    "print(f\"Final test labels shape: {y_test_seq_onehot.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc05b4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a53e5af5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T13:01:06.292097Z",
     "start_time": "2025-04-26T13:01:06.271011Z"
    }
   },
   "outputs": [],
   "source": [
    "class MST_Block(layers.Layer):\n",
    "    def __init__(self, filters, kernel_sizes=(3, 5, 7), **kwargs):\n",
    "        super(MST_Block, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        \n",
    "        # For each kernel size, create a depthwise conv layer\n",
    "        self.conv_layers = []\n",
    "        \n",
    "        # Follow the paper's guideline for dimension allocation\n",
    "        if len(kernel_sizes) == 3:  # Stage-1 with 3 kernel sizes\n",
    "            self.filter_dims = [filters//4, filters//4, filters//2]  # 32, 32, 64 for filters=128\n",
    "        else:  # Stage-2/3 with 2 kernel sizes\n",
    "            self.filter_dims = [filters//2, filters//2]  # 각각 64로 설정하여 합이 128이 되도록 수정\n",
    "        \n",
    "        # Create depthwise convolution for each kernel size\n",
    "        for i, k_size in enumerate(kernel_sizes):\n",
    "            self.conv_layers.append(\n",
    "                layers.DepthwiseConv1D(\n",
    "                    kernel_size=k_size,\n",
    "                    strides=1,\n",
    "                    padding='same',\n",
    "                    depth_multiplier=1,\n",
    "                    activation=None,\n",
    "                    name=f'dwconv_{k_size}'\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Projection layers after each convolution\n",
    "        self.proj_layers = []\n",
    "        for i in range(len(kernel_sizes)):\n",
    "            self.proj_layers.append(\n",
    "                layers.Conv1D(\n",
    "                    filters=self.filter_dims[i],\n",
    "                    kernel_size=1,\n",
    "                    activation='relu',\n",
    "                    name=f'proj_{self.kernel_sizes[i]}'\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Window-based self-attention (W-SA) with relative position bias\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=4, \n",
    "            key_dim=filters // 4\n",
    "        )\n",
    "        \n",
    "        # 출력 차원을 명확하게 일치시키기 위한 투영 레이어 추가\n",
    "        self.output_proj = layers.Conv1D(filters, kernel_size=1, activation=None)\n",
    "        \n",
    "        # MLP after attention\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            layers.Dense(filters * 4, activation='relu'),\n",
    "            layers.Dense(filters)\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Multi-scale feature aggregation\n",
    "        x = inputs\n",
    "        feature_maps = []\n",
    "        \n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "            # Apply depthwise convolution\n",
    "            conv_out = conv_layer(x)\n",
    "            # Apply projection\n",
    "            proj_out = self.proj_layers[i](conv_out)\n",
    "            feature_maps.append(proj_out)\n",
    "        \n",
    "        # Concatenate along feature dimension\n",
    "        concat_features = tf.concat(feature_maps, axis=-1)\n",
    "        \n",
    "        # 출력 차원을 입력 차원과 정확히 일치시키기 위해 투영\n",
    "        x = self.output_proj(concat_features)\n",
    "        \n",
    "        # Apply window-based self-attention with relative position bias\n",
    "        attn_input = self.layer_norm1(x)\n",
    "        attention_output = self.attention(attn_input, attn_input)\n",
    "        x = x + attention_output\n",
    "        \n",
    "        # Apply MLP with residual connection\n",
    "        mlp_input = self.layer_norm2(x)\n",
    "        mlp_output = self.mlp(mlp_input)\n",
    "        return x + mlp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "36365b5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T13:01:06.313588Z",
     "start_time": "2025-04-26T13:01:06.299689Z"
    }
   },
   "outputs": [],
   "source": [
    "class PAT_Block(layers.Layer):\n",
    "    def __init__(self, num_bases=96, **kwargs):\n",
    "        super(PAT_Block, self).__init__(**kwargs)\n",
    "        self.num_bases = num_bases\n",
    "        \n",
    "        # Linear layer for query transformation\n",
    "        self.query_proj = layers.Dense(128, name='query_proj')\n",
    "        \n",
    "        # MLP for base estimation with linear layer and ReLU\n",
    "        self.base_mlp = tf.keras.Sequential([\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(128)\n",
    "        ], name='base_mlp')\n",
    "        \n",
    "        # Final MLP for output\n",
    "        self.final_mlp = tf.keras.Sequential([\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(128)\n",
    "        ], name='final_mlp')\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # For simplicity, we'll use fixed bases for all samples in a batch\n",
    "        # In a complete implementation, this should be dynamic using K-means per sample\n",
    "        self.bases = self.add_weight(\n",
    "            name='bases',\n",
    "            shape=(self.num_bases, input_shape[-1]),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        super(PAT_Block, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Calculate queries from input\n",
    "        queries = self.query_proj(inputs)\n",
    "        \n",
    "        # Process bases through the MLP\n",
    "        processed_bases = self.base_mlp(self.bases)\n",
    "        \n",
    "        # Calculate attention scores between queries and bases\n",
    "        # Shape: (batch_size, sequence_length, num_bases)\n",
    "        attention_scores = tf.matmul(queries, tf.transpose(processed_bases))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        \n",
    "        # Calculate re-estimated features by weighted sum of bases\n",
    "        # Shape: (batch_size, sequence_length, feature_dim)\n",
    "        reestimated = tf.matmul(attention_weights, processed_bases)\n",
    "        \n",
    "        # Calculate feature difference (like PCT paper)\n",
    "        feature_diff = reestimated - inputs\n",
    "        \n",
    "        # Apply final MLP and residual connection\n",
    "        output = inputs + self.final_mlp(feature_diff)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "792f015d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T13:01:06.328623Z",
     "start_time": "2025-04-26T13:01:06.319429Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_patchformer_model(input_shape, num_classes=6):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Initial embedding using shared MLP (implemented as Conv1D)\n",
    "    x = layers.Conv1D(128, kernel_size=1, activation='relu')(inputs)\n",
    "    \n",
    "    # Stage 1: MST Block + PAT Block\n",
    "    # MST Block with 3 kernel sizes (3, 5, 7)\n",
    "    x = MST_Block(filters=128, kernel_sizes=(3, 5, 7))(x)\n",
    "    x = PAT_Block(num_bases=96)(x)\n",
    "    \n",
    "    # Stage 2: MST Block + PAT Block\n",
    "    # MST Block with 2 kernel sizes (3, 5)\n",
    "    x = MST_Block(filters=128, kernel_sizes=(3, 5))(x)\n",
    "    x = PAT_Block(num_bases=96)(x)\n",
    "    \n",
    "    # Stage 3: MST Block + PAT Block\n",
    "    # MST Block with 2 kernel sizes (3, 5)\n",
    "    x = MST_Block(filters=128, kernel_sizes=(3, 5))(x)\n",
    "    x = PAT_Block(num_bases=96)(x)\n",
    "    \n",
    "    # Global pooling\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Classification head\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d7cd8408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T13:01:09.694767Z",
     "start_time": "2025-04-26T13:01:06.333446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_53\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_53\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mst__block_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MST_Block</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">233,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pat__block_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PAT_Block</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">94,848</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mst__block_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MST_Block</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">232,576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pat__block_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PAT_Block</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">94,848</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mst__block_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MST_Block</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">232,576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pat__block_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PAT_Block</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">94,848</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_11     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_245 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_246 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_55 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m9\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_40 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m1,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mst__block_13 (\u001b[38;5;33mMST_Block\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m233,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pat__block_7 (\u001b[38;5;33mPAT_Block\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m94,848\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mst__block_14 (\u001b[38;5;33mMST_Block\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m232,576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pat__block_8 (\u001b[38;5;33mPAT_Block\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m94,848\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mst__block_15 (\u001b[38;5;33mMST_Block\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m232,576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pat__block_9 (\u001b[38;5;33mPAT_Block\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m94,848\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_11     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_245 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_28 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_246 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m774\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,001,862</span> (3.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,001,862\u001b[0m (3.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,001,862</span> (3.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,001,862\u001b[0m (3.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_patchformer_model(input_shape=(window_size, x_train.shape[2]))\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52a58d3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-26T13:01:00.613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 892ms/step - accuracy: 0.5000 - loss: 1.1115 - val_accuracy: 0.8001 - val_loss: 0.4512 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 886ms/step - accuracy: 0.8122 - loss: 0.4629 - val_accuracy: 0.9334 - val_loss: 0.1654 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 966ms/step - accuracy: 0.9294 - loss: 0.1885 - val_accuracy: 0.9069 - val_loss: 0.2437 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 834ms/step - accuracy: 0.9439 - loss: 0.1463 - val_accuracy: 0.9551 - val_loss: 0.1112 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 1s/step - accuracy: 0.9459 - loss: 0.1393 - val_accuracy: 0.8899 - val_loss: 0.5412 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 971ms/step - accuracy: 0.9420 - loss: 0.1640 - val_accuracy: 0.9443 - val_loss: 0.1173 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 938ms/step - accuracy: 0.9463 - loss: 0.1308 - val_accuracy: 0.9517 - val_loss: 0.1313 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 912ms/step - accuracy: 0.9517 - loss: 0.1251 - val_accuracy: 0.9545 - val_loss: 0.1112 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 811ms/step - accuracy: 0.9467 - loss: 0.1287 - val_accuracy: 0.9524 - val_loss: 0.1139 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 778ms/step - accuracy: 0.9524 - loss: 0.0982 - val_accuracy: 0.9585 - val_loss: 0.0907 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m 97/184\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 770ms/step - accuracy: 0.9584 - loss: 0.0898"
     ]
    }
   ],
   "source": [
    "x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    x_train_seq, y_train_seq_onehot, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5, \n",
    "    patience=5, \n",
    "    min_lr=1e-5\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_split,\n",
    "    y_train_split,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val_split, y_val_split),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d00ad",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-26T13:01:00.615Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2eb0d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-26T13:01:00.617Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test_seq, y_test_seq_onehot)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "\n",
    "y_pred_proba = model.predict(x_test_seq)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_true = np.argmax(y_test_seq_onehot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f6e0b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-26T13:01:00.619Z"
    }
   },
   "outputs": [],
   "source": [
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Calculate specificity for each class\n",
    "def specificity_score(y_true, y_pred, num_classes=6):\n",
    "    specificities = []\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        true_negative = np.sum((y_true != i) & (y_pred != i))\n",
    "        false_positive = np.sum((y_true != i) & (y_pred == i))\n",
    "        \n",
    "        if true_negative + false_positive == 0:\n",
    "            specificities.append(1.0)\n",
    "        else:\n",
    "            specificities.append(true_negative / (true_negative + false_positive))\n",
    "    \n",
    "    return specificities\n",
    "\n",
    "specificities = specificity_score(y_true, y_pred)\n",
    "avg_specificity = np.mean(specificities)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Average Specificity: {avg_specificity:.4f}\")\n",
    "\n",
    "print(\"\\nClass-wise Specificities:\")\n",
    "for i, spec in enumerate(specificities):\n",
    "    print(f\"Class {i} ({activity_labels.iloc[i, 1]}): {spec:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbaa749",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-26T13:01:00.621Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, \n",
    "                           target_names=activity_labels['class_name'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf6e6ee",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-26T13:01:00.623Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=activity_labels['class_name'].values,\n",
    "           yticklabels=activity_labels['class_name'].values)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - PatchFormer (UCI-HAR)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
