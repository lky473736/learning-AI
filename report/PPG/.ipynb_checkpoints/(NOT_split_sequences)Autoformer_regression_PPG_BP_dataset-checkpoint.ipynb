{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a82a82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T04:17:39.870644Z",
     "start_time": "2025-04-02T04:17:16.688355Z"
    },
    "id": "52a82a82"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a72c36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T04:17:57.045165Z",
     "start_time": "2025-04-02T04:17:39.873336Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4a72c36",
    "outputId": "7824f7cb-01d2-4977-ed38-f5c02f39f555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 657 entries, 0 to 656\n",
      "Columns: 2114 entries, Num. to 2100\n",
      "dtypes: float64(1), int64(2108), object(5)\n",
      "memory usage: 10.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../data/combined_dataset-1.xlsx'\n",
    "df = pd.read_excel(data_path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc8fbad1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T04:17:57.066004Z",
     "start_time": "2025-04-02T04:17:57.046904Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "cc8fbad1",
    "outputId": "053c0e7b-2d02-4e4a-949f-4adb15d712b8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num.</th>\n",
       "      <th>subject_ID</th>\n",
       "      <th>Sex(M/F)</th>\n",
       "      <th>Age(year)</th>\n",
       "      <th>Height(cm)</th>\n",
       "      <th>Weight(kg)</th>\n",
       "      <th>Systolic Blood Pressure(mmHg)</th>\n",
       "      <th>Diastolic Blood Pressure(mmHg)</th>\n",
       "      <th>Heart Rate(b/m)</th>\n",
       "      <th>BMI(kg/m^2)</th>\n",
       "      <th>...</th>\n",
       "      <th>2091</th>\n",
       "      <th>2092</th>\n",
       "      <th>2093</th>\n",
       "      <th>2094</th>\n",
       "      <th>2095</th>\n",
       "      <th>2096</th>\n",
       "      <th>2097</th>\n",
       "      <th>2098</th>\n",
       "      <th>2099</th>\n",
       "      <th>2100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "      <td>45</td>\n",
       "      <td>152</td>\n",
       "      <td>63</td>\n",
       "      <td>161</td>\n",
       "      <td>89</td>\n",
       "      <td>97</td>\n",
       "      <td>27.268006</td>\n",
       "      <td>...</td>\n",
       "      <td>1766</td>\n",
       "      <td>1766</td>\n",
       "      <td>1766</td>\n",
       "      <td>1833</td>\n",
       "      <td>1833</td>\n",
       "      <td>1827</td>\n",
       "      <td>1827</td>\n",
       "      <td>1827</td>\n",
       "      <td>1754</td>\n",
       "      <td>1754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "      <td>45</td>\n",
       "      <td>152</td>\n",
       "      <td>63</td>\n",
       "      <td>161</td>\n",
       "      <td>89</td>\n",
       "      <td>97</td>\n",
       "      <td>27.268006</td>\n",
       "      <td>...</td>\n",
       "      <td>1985</td>\n",
       "      <td>1985</td>\n",
       "      <td>2026</td>\n",
       "      <td>2026</td>\n",
       "      <td>2026</td>\n",
       "      <td>1977</td>\n",
       "      <td>1977</td>\n",
       "      <td>1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "      <td>45</td>\n",
       "      <td>152</td>\n",
       "      <td>63</td>\n",
       "      <td>161</td>\n",
       "      <td>89</td>\n",
       "      <td>97</td>\n",
       "      <td>27.268006</td>\n",
       "      <td>...</td>\n",
       "      <td>1942</td>\n",
       "      <td>1900</td>\n",
       "      <td>1900</td>\n",
       "      <td>1938</td>\n",
       "      <td>1938</td>\n",
       "      <td>1938</td>\n",
       "      <td>1924</td>\n",
       "      <td>1924</td>\n",
       "      <td>1929</td>\n",
       "      <td>1929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Female</td>\n",
       "      <td>50</td>\n",
       "      <td>157</td>\n",
       "      <td>50</td>\n",
       "      <td>160</td>\n",
       "      <td>93</td>\n",
       "      <td>76</td>\n",
       "      <td>20.284799</td>\n",
       "      <td>...</td>\n",
       "      <td>2073</td>\n",
       "      <td>2072</td>\n",
       "      <td>2072</td>\n",
       "      <td>2072</td>\n",
       "      <td>2051</td>\n",
       "      <td>2051</td>\n",
       "      <td>2036</td>\n",
       "      <td>2036</td>\n",
       "      <td>2036</td>\n",
       "      <td>2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Female</td>\n",
       "      <td>50</td>\n",
       "      <td>157</td>\n",
       "      <td>50</td>\n",
       "      <td>160</td>\n",
       "      <td>93</td>\n",
       "      <td>76</td>\n",
       "      <td>20.284799</td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010</td>\n",
       "      <td>2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Female</td>\n",
       "      <td>50</td>\n",
       "      <td>157</td>\n",
       "      <td>50</td>\n",
       "      <td>160</td>\n",
       "      <td>93</td>\n",
       "      <td>76</td>\n",
       "      <td>20.284799</td>\n",
       "      <td>...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020</td>\n",
       "      <td>2032</td>\n",
       "      <td>2032</td>\n",
       "      <td>2032</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>Female</td>\n",
       "      <td>47</td>\n",
       "      <td>150</td>\n",
       "      <td>47</td>\n",
       "      <td>101</td>\n",
       "      <td>71</td>\n",
       "      <td>79</td>\n",
       "      <td>20.888889</td>\n",
       "      <td>...</td>\n",
       "      <td>2047</td>\n",
       "      <td>2047</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>2053</td>\n",
       "      <td>2053</td>\n",
       "      <td>2038</td>\n",
       "      <td>2038</td>\n",
       "      <td>2038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>Female</td>\n",
       "      <td>47</td>\n",
       "      <td>150</td>\n",
       "      <td>47</td>\n",
       "      <td>101</td>\n",
       "      <td>71</td>\n",
       "      <td>79</td>\n",
       "      <td>20.888889</td>\n",
       "      <td>...</td>\n",
       "      <td>2076</td>\n",
       "      <td>2076</td>\n",
       "      <td>2051</td>\n",
       "      <td>2051</td>\n",
       "      <td>2051</td>\n",
       "      <td>2060</td>\n",
       "      <td>2060</td>\n",
       "      <td>2067</td>\n",
       "      <td>2067</td>\n",
       "      <td>2067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>Female</td>\n",
       "      <td>47</td>\n",
       "      <td>150</td>\n",
       "      <td>47</td>\n",
       "      <td>101</td>\n",
       "      <td>71</td>\n",
       "      <td>79</td>\n",
       "      <td>20.888889</td>\n",
       "      <td>...</td>\n",
       "      <td>2163</td>\n",
       "      <td>2159</td>\n",
       "      <td>2159</td>\n",
       "      <td>2159</td>\n",
       "      <td>2175</td>\n",
       "      <td>2175</td>\n",
       "      <td>2168</td>\n",
       "      <td>2168</td>\n",
       "      <td>2168</td>\n",
       "      <td>2175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>172</td>\n",
       "      <td>65</td>\n",
       "      <td>136</td>\n",
       "      <td>93</td>\n",
       "      <td>87</td>\n",
       "      <td>21.971336</td>\n",
       "      <td>...</td>\n",
       "      <td>1985</td>\n",
       "      <td>1985</td>\n",
       "      <td>1985</td>\n",
       "      <td>1984</td>\n",
       "      <td>1984</td>\n",
       "      <td>1995</td>\n",
       "      <td>1995</td>\n",
       "      <td>1995</td>\n",
       "      <td>1972</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 2114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Num.  subject_ID Sex(M/F)  Age(year)  Height(cm)  Weight(kg)  \\\n",
       "0     1           2   Female         45         152          63   \n",
       "1     1           2   Female         45         152          63   \n",
       "2     1           2   Female         45         152          63   \n",
       "3     2           3   Female         50         157          50   \n",
       "4     2           3   Female         50         157          50   \n",
       "5     2           3   Female         50         157          50   \n",
       "6     3           6   Female         47         150          47   \n",
       "7     3           6   Female         47         150          47   \n",
       "8     3           6   Female         47         150          47   \n",
       "9     4           8     Male         45         172          65   \n",
       "\n",
       "   Systolic Blood Pressure(mmHg)  Diastolic Blood Pressure(mmHg)  \\\n",
       "0                            161                              89   \n",
       "1                            161                              89   \n",
       "2                            161                              89   \n",
       "3                            160                              93   \n",
       "4                            160                              93   \n",
       "5                            160                              93   \n",
       "6                            101                              71   \n",
       "7                            101                              71   \n",
       "8                            101                              71   \n",
       "9                            136                              93   \n",
       "\n",
       "   Heart Rate(b/m)  BMI(kg/m^2)  ...  2091  2092  2093  2094  2095  2096  \\\n",
       "0               97    27.268006  ...  1766  1766  1766  1833  1833  1827   \n",
       "1               97    27.268006  ...  1985  1985  2026  2026  2026  1977   \n",
       "2               97    27.268006  ...  1942  1900  1900  1938  1938  1938   \n",
       "3               76    20.284799  ...  2073  2072  2072  2072  2051  2051   \n",
       "4               76    20.284799  ...  2021  2010  2010  2010  2001  2001   \n",
       "5               76    20.284799  ...  2020  2020  2032  2032  2032  2011   \n",
       "6               79    20.888889  ...  2047  2047  2017  2017  2017  2053   \n",
       "7               79    20.888889  ...  2076  2076  2051  2051  2051  2060   \n",
       "8               79    20.888889  ...  2163  2159  2159  2159  2175  2175   \n",
       "9               87    21.971336  ...  1985  1985  1985  1984  1984  1995   \n",
       "\n",
       "   2097  2098  2099  2100  \n",
       "0  1827  1827  1754  1754  \n",
       "1  1977  1997  1997  1997  \n",
       "2  1924  1924  1929  1929  \n",
       "3  2036  2036  2036  2045  \n",
       "4  2003  2003  2003  1989  \n",
       "5  2011  2005  2005  2005  \n",
       "6  2053  2038  2038  2038  \n",
       "7  2060  2067  2067  2067  \n",
       "8  2168  2168  2168  2175  \n",
       "9  1995  1995  1972  1972  \n",
       "\n",
       "[10 rows x 2114 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e97c6de7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T04:17:59.396774Z",
     "start_time": "2025-04-02T04:17:57.069056Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "e97c6de7",
    "outputId": "a7070e93-6741-4832-d398-f29843e31c37"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Num.</th>\n",
       "      <td>657.0</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>63.267362</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject_ID</th>\n",
       "      <td>657.0</td>\n",
       "      <td>156.598174</td>\n",
       "      <td>101.449344</td>\n",
       "      <td>2.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>419.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age(year)</th>\n",
       "      <td>657.0</td>\n",
       "      <td>57.168950</td>\n",
       "      <td>15.850110</td>\n",
       "      <td>21.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Height(cm)</th>\n",
       "      <td>657.0</td>\n",
       "      <td>161.228311</td>\n",
       "      <td>8.190357</td>\n",
       "      <td>145.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weight(kg)</th>\n",
       "      <td>657.0</td>\n",
       "      <td>60.191781</td>\n",
       "      <td>11.868168</td>\n",
       "      <td>36.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>657.0</td>\n",
       "      <td>2085.436834</td>\n",
       "      <td>305.845135</td>\n",
       "      <td>1519.0</td>\n",
       "      <td>1904.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>2180.0</td>\n",
       "      <td>3811.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>657.0</td>\n",
       "      <td>2083.791476</td>\n",
       "      <td>304.297297</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>1904.0</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>2176.0</td>\n",
       "      <td>3787.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>657.0</td>\n",
       "      <td>2084.803653</td>\n",
       "      <td>306.657540</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>1904.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>2175.0</td>\n",
       "      <td>3774.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>657.0</td>\n",
       "      <td>2085.196347</td>\n",
       "      <td>306.275406</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>2177.0</td>\n",
       "      <td>3775.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>657.0</td>\n",
       "      <td>2085.219178</td>\n",
       "      <td>305.731730</td>\n",
       "      <td>1528.0</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2183.0</td>\n",
       "      <td>3759.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2109 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            count         mean         std     min     25%     50%     75%  \\\n",
       "Num.        657.0   110.000000   63.267362     1.0    55.0   110.0   165.0   \n",
       "subject_ID  657.0   156.598174  101.449344     2.0    85.0   152.0   215.0   \n",
       "Age(year)   657.0    57.168950   15.850110    21.0    48.0    58.0    68.0   \n",
       "Height(cm)  657.0   161.228311    8.190357   145.0   155.0   160.0   167.0   \n",
       "Weight(kg)  657.0    60.191781   11.868168    36.0    52.0    60.0    67.0   \n",
       "...           ...          ...         ...     ...     ...     ...     ...   \n",
       "2096        657.0  2085.436834  305.845135  1519.0  1904.0  2014.0  2180.0   \n",
       "2097        657.0  2083.791476  304.297297  1515.0  1904.0  2012.0  2176.0   \n",
       "2098        657.0  2084.803653  306.657540  1515.0  1904.0  2011.0  2175.0   \n",
       "2099        657.0  2085.196347  306.275406  1515.0  1906.0  2012.0  2177.0   \n",
       "2100        657.0  2085.219178  305.731730  1528.0  1906.0  2017.0  2183.0   \n",
       "\n",
       "               max  \n",
       "Num.         219.0  \n",
       "subject_ID   419.0  \n",
       "Age(year)     86.0  \n",
       "Height(cm)   196.0  \n",
       "Weight(kg)   103.0  \n",
       "...            ...  \n",
       "2096        3811.0  \n",
       "2097        3787.0  \n",
       "2098        3774.0  \n",
       "2099        3775.0  \n",
       "2100        3759.0  \n",
       "\n",
       "[2109 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ba3663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T04:17:59.403298Z",
     "start_time": "2025-04-02T04:17:59.398899Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0ba3663",
    "outputId": "f976cef6-87a8-4482-8011-fe2a19a6491d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(657, 2114)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "u98HtrbxHZR2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T04:17:59.428845Z",
     "start_time": "2025-04-02T04:17:59.405224Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u98HtrbxHZR2",
    "outputId": "cd03a850-8696-4907-bf6a-0ed7f2f04e0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 597, 582, 543}\n",
      "{0, 75, 72, 174}\n",
      "{0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/gryfr07n59jgb3wrd062h1ym0000gn/T/ipykernel_10182/4277235848.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='ffill')\n",
      "/var/folders/_z/gryfr07n59jgb3wrd062h1ym0000gn/T/ipykernel_10182/4277235848.py:4: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "print (set(df.isnull().sum()))\n",
    "df = df.fillna(method='ffill')\n",
    "print (set(df.isnull().sum()))\n",
    "df = df.fillna(method='bfill')\n",
    "print (set(df.isnull().sum()))\n",
    "df = df.fillna(df.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc891c65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T04:17:59.434816Z",
     "start_time": "2025-04-02T04:17:59.430979Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc891c65",
    "outputId": "81792d38-bf8f-4713-d5ff-7ec7d94fe1d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata col # : ['Num.', 'subject_ID', 'Sex(M/F)', 'Age(year)', 'Height(cm)', 'Weight(kg)', 'Systolic Blood Pressure(mmHg)', 'Diastolic Blood Pressure(mmHg)', 'Heart Rate(b/m)', 'BMI(kg/m^2)', 'Hypertension', 'Diabetes', 'cerebral infarction', 'cerebrovascular disease']\n",
      "signal data col # : 2100\n"
     ]
    }
   ],
   "source": [
    "meta_end_idx = df.columns.get_loc('cerebrovascular disease') + 1\n",
    "meta_cols = df.columns[:meta_end_idx]\n",
    "signal_cols = df.columns[meta_end_idx:]\n",
    "\n",
    "print(f\"metadata col # : {list(meta_cols)}\")\n",
    "print(f\"signal data col # : {len(signal_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c05b42c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T04:17:59.447197Z",
     "start_time": "2025-04-02T04:17:59.437400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c05b42c8",
    "outputId": "93247531-fb75-4839-9450-b25b02096b8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal data : (657, 2100)\n",
      "pressure data : (657, 2)\n"
     ]
    }
   ],
   "source": [
    "target_cols = ['Systolic Blood Pressure(mmHg)', 'Diastolic Blood Pressure(mmHg)']\n",
    "\n",
    "X_signals = df[signal_cols].values\n",
    "y_bp = df[target_cols].values\n",
    "\n",
    "print(f\"signal data : {X_signals.shape}\")\n",
    "print(f\"pressure data : {y_bp.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62ada5cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T04:17:59.481516Z",
     "start_time": "2025-04-02T04:17:59.449105Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62ada5cc",
    "outputId": "04c6369b-3e32-47a6-9f25-21cd2771b0dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal data : (657, 2100)\n",
      "pressure data : (657, 2)\n",
      "train=(420, 2100), val=(105, 2100), test=(132, 2100)\n"
     ]
    }
   ],
   "source": [
    "X_data = X_signals  # Shape: [num_samples, signal_length]\n",
    "y_data = y_bp      # Shape: [num_samples, 2]\n",
    "\n",
    "print(f\"signal data : {X_data.shape}\")\n",
    "print(f\"pressure data : {y_data.shape}\")\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"train={X_train.shape}, val={X_val.shape}, test={X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516c0737",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T04:17:59.517994Z",
     "start_time": "2025-04-02T04:17:59.486734Z"
    },
    "id": "516c0737"
   },
   "outputs": [],
   "source": [
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train.reshape(X_train.shape[0], -1)).reshape(X_train.shape)\n",
    "X_val = scaler_X.transform(X_val.reshape(X_val.shape[0], -1)).reshape(X_val.shape)\n",
    "X_test = scaler_X.transform(X_test.reshape(X_test.shape[0], -1)).reshape(X_test.shape)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "y_val = scaler_y.transform(y_val)\n",
    "y_test = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d8eeb5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T04:17:59.562494Z",
     "start_time": "2025-04-02T04:17:59.520359Z"
    },
    "id": "5d8eeb5a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.norm(x, dim=-1, keepdim=True) * (x.shape[-1] ** -0.5)\n",
    "        return x / (norm + self.eps) * self.weight\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Use Conv1d to handle the input shape [batch, 1, seq_len]\n",
    "        self.proj = nn.Conv1d(in_channels=1, out_channels=d_model, kernel_size=1)\n",
    "        self.norm = nn.BatchNorm1d(d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, 1, seq_len]\n",
    "        x = self.proj(x)  # [batch, d_model, seq_len]\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(1, 2)  # [batch, seq_len, d_model]\n",
    "        return x\n",
    "\n",
    "class SeriesDecomp(nn.Module):\n",
    "    \"\"\"Time series decomposition module - separates trend and seasonality\"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        # Ensure kernel size is odd for symmetric padding\n",
    "        if kernel_size % 2 == 0:\n",
    "            self.kernel_size = kernel_size + 1\n",
    "\n",
    "        self.avg = nn.AvgPool1d(\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=1,\n",
    "            padding=self.kernel_size//2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Length, Channel]\n",
    "        x_transpose = x.transpose(1, 2)  # [Batch, Channel, Length]\n",
    "\n",
    "        # Extract trend using average pooling\n",
    "        trend = self.avg(x_transpose).transpose(1, 2)  # [Batch, Length, Channel]\n",
    "\n",
    "        # Original - trend = seasonal component\n",
    "        seasonal = x - trend\n",
    "\n",
    "        return seasonal, trend\n",
    "\n",
    "class AutoCorrelation(nn.Module):\n",
    "    \"\"\"Auto-correlation mechanism with improved stability\"\"\"\n",
    "    def __init__(self, dim, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = dim // heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        # q, k, v: [Batch, Length, Dim]\n",
    "        batch_size, q_len, _ = queries.shape\n",
    "        _, k_len, _ = keys.shape\n",
    "\n",
    "        # Project and reshape for multi-head\n",
    "        q = self.q_proj(queries).view(batch_size, q_len, self.heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(keys).view(batch_size, k_len, self.heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(values).view(batch_size, k_len, self.heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Use a simpler and more stable attention mechanism instead of FFT\n",
    "        # This avoids potential NaN issues with FFT operations\n",
    "        attn = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        # Apply masking if provided\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attn, v)\n",
    "\n",
    "        # Reshape back to original dimensions\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, q_len, self.dim)\n",
    "\n",
    "        # Project output\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class AutoformerEncoderLayer(nn.Module):\n",
    "    \"\"\"Autoformer Encoder Layer with auto-correlation and decomposition\"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff=None, dropout=0.1, activation=\"relu\", decomp_kernel=5):\n",
    "        super().__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "\n",
    "        # Auto-correlation mechanism\n",
    "        self.autocorr = AutoCorrelation(d_model, heads=n_heads, dropout=dropout)\n",
    "\n",
    "        # Series decomposition\n",
    "        self.decomp1 = SeriesDecomp(decomp_kernel)\n",
    "        self.decomp2 = SeriesDecomp(decomp_kernel)\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU() if activation == \"relu\" else nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Auto-correlation Block\n",
    "        autocorr_output = self.autocorr(x, x, x, mask=mask)\n",
    "\n",
    "        # First residual connection\n",
    "        x1 = autocorr_output + x\n",
    "\n",
    "        # Decomposition - split seasonal & trend\n",
    "        x_seasonal, x_trend = self.decomp1(x1)\n",
    "\n",
    "        # Feed-forward Block\n",
    "        ff_output = self.ff(x_seasonal)\n",
    "\n",
    "        # Second residual connection\n",
    "        x2 = ff_output + x_seasonal\n",
    "\n",
    "        # Decomposition\n",
    "        out_seasonal, out_trend = self.decomp2(x2)\n",
    "\n",
    "        # Final output = seasonal + trend\n",
    "        output = out_seasonal + x_trend\n",
    "\n",
    "        return output\n",
    "\n",
    "class AutoformerEncoder(nn.Module):\n",
    "    \"\"\"Autoformer encoder with stacked encoder layers\"\"\"\n",
    "    def __init__(self, d_model, n_layers, n_heads, d_ff=None, dropout=0.1, activation=\"relu\", decomp_kernel=5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            AutoformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                n_heads=n_heads,\n",
    "                d_ff=d_ff,\n",
    "                dropout=dropout,\n",
    "                activation=activation,\n",
    "                decomp_kernel=decomp_kernel\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: [Batch, Length, Dim]\n",
    "\n",
    "        # Apply encoder layers sequentially\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask)\n",
    "\n",
    "        # Apply final normalization\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EnhancedAutoformer(nn.Module):\n",
    "    \"\"\"Enhanced Autoformer model with CNN layers for improved performance\"\"\"\n",
    "    def __init__(self, d_model=64, n_layers=4, n_heads=8, d_ff=None, dropout=0.1,\n",
    "                 decomp_kernel=5, activation=\"relu\", cnn_channels=[128, 64]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Data embedding\n",
    "        self.embedding = DataEmbedding(d_model, dropout)\n",
    "\n",
    "        # Autoformer encoder\n",
    "        self.encoder = AutoformerEncoder(\n",
    "            d_model=d_model,\n",
    "            n_layers=n_layers,\n",
    "            n_heads=n_heads,\n",
    "            d_ff=d_ff,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            decomp_kernel=decomp_kernel\n",
    "        )\n",
    "\n",
    "        # CNN feature extraction after Autoformer\n",
    "        self.post_cnn_layers = nn.ModuleList()\n",
    "\n",
    "        # Current channels is d_model\n",
    "        in_channels = d_model\n",
    "\n",
    "        # Add CNN layers\n",
    "        for out_channels in cnn_channels:\n",
    "            self.post_cnn_layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "                )\n",
    "            )\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # Calculate CNN output size (depends on input sequence length)\n",
    "        self.calc_cnn_output_size = lambda seq_len: seq_len // (2 ** len(cnn_channels))\n",
    "\n",
    "        # Global pooling for variable length sequences\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Output head for blood pressure prediction with additional features\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(cnn_channels[-1] + d_model * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, 1, seq_len]\n",
    "        batch_size, _, seq_len = x.shape\n",
    "\n",
    "        # Apply embedding\n",
    "        embedded = self.embedding(x)  # [batch, seq_len, d_model]\n",
    "\n",
    "        # Apply Autoformer encoder\n",
    "        encoded = self.encoder(embedded)  # [batch, seq_len, d_model]\n",
    "\n",
    "        # Autoformer features: mean and max pooling\n",
    "        mean_pooled = torch.mean(encoded, dim=1)  # [batch, d_model]\n",
    "        max_pooled, _ = torch.max(encoded, dim=1)  # [batch, d_model]\n",
    "\n",
    "        # Apply CNN layers for additional feature extraction\n",
    "        # Need to change dimensions for CNN\n",
    "        cnn_input = encoded.transpose(1, 2)  # [batch, d_model, seq_len]\n",
    "\n",
    "        for cnn_layer in self.post_cnn_layers:\n",
    "            cnn_input = cnn_layer(cnn_input)\n",
    "\n",
    "        # Global pooling to handle variable sequence lengths\n",
    "        cnn_features = self.global_pool(cnn_input).squeeze(-1)  # [batch, cnn_channels[-1]]\n",
    "\n",
    "        # Concatenate all features\n",
    "        combined_features = torch.cat([mean_pooled, max_pooled, cnn_features], dim=-1)\n",
    "\n",
    "        # Project to blood pressure values\n",
    "        bp_pred = self.projection(combined_features)  # [batch, 2]\n",
    "\n",
    "        return bp_pred\n",
    "\n",
    "# Dataset class\n",
    "class BPDataset(Dataset):\n",
    "    def __init__(self, signals, bp_values):\n",
    "        self.signals = signals\n",
    "        self.bp_values = bp_values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signal = self.signals[idx]\n",
    "        bp = self.bp_values[idx]\n",
    "\n",
    "        # Convert to [1, sequence_length] shape\n",
    "        x = torch.FloatTensor(signal).unsqueeze(0)\n",
    "        y = torch.FloatTensor(bp)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "# Custom loss function combining MSE with correlation loss\n",
    "class BPLoss(nn.Module):\n",
    "    def __init__(self, mse_weight=1.0, corr_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.mse_weight = mse_weight\n",
    "        self.corr_weight = corr_weight\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # MSE Loss\n",
    "        mse = self.mse_loss(pred, target)\n",
    "\n",
    "        # Correlation Loss (encourages predictions to follow the same trend as targets)\n",
    "        pred_centered = pred - pred.mean(dim=0, keepdim=True)\n",
    "        target_centered = target - target.mean(dim=0, keepdim=True)\n",
    "\n",
    "        pred_norm = torch.norm(pred_centered, dim=0)\n",
    "        target_norm = torch.norm(target_centered, dim=0)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        pred_norm = torch.clamp(pred_norm, min=1e-8)\n",
    "        target_norm = torch.clamp(target_norm, min=1e-8)\n",
    "\n",
    "        # Calculate correlation for both SBP and DBP\n",
    "        corr = (pred_centered * target_centered).sum(dim=0) / (pred_norm * target_norm)\n",
    "\n",
    "        # Convert correlation to loss (1 - mean correlation)\n",
    "        corr_loss = 1 - corr.mean()\n",
    "\n",
    "        # Combined loss\n",
    "        combined_loss = self.mse_weight * mse + self.corr_weight * corr_loss\n",
    "\n",
    "        return combined_loss\n",
    "\n",
    "# Training function with gradient clipping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=50, device='cuda'):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for x, y in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]'):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping to prevent explosion\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Valid]'):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "                val_loss += loss.item() * x.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_enhanced_autoformer.pth')\n",
    "            print(f'Epoch {epoch+1}: model saved (val_loss: {val_loss:.4f})')\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6921cf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T04:17:59.569200Z",
     "start_time": "2025-04-02T04:17:59.564612Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6921cf9",
    "outputId": "1d524d8b-24c8-43b4-94cf-4e5eedf2dce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train=420, val=105, test=132\n",
      "train=14, val=4, test=5\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BPDataset(X_train, y_train)\n",
    "val_dataset = BPDataset(X_val, y_val)\n",
    "test_dataset = BPDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"train={len(train_dataset)}, val={len(val_dataset)}, test={len(test_dataset)}\") # <--- data point\n",
    "print(f\"train={len(train_loader)}, val={len(val_loader)}, test={len(test_loader)}\") # <-- batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec8c19",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-02T04:18:04.296Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "a8ec8c19",
    "outputId": "572c646e-f580-4197-ba87-7bcc36a409b4"
   },
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "num_epochs = 150\n",
    "\n",
    "model = EnhancedAutoformer(\n",
    "    d_model=64,         # 모델 차원\n",
    "    n_layers=4,         # Autoformer 레이어 수\n",
    "    n_heads=8,          # 어텐션 헤드 수\n",
    "    d_ff=256,           # 피드포워드 네트워크 차원\n",
    "    dropout=0.1,        # 드롭아웃 비율\n",
    "    decomp_kernel=5,    # 시계열 분해 커널 크기\n",
    "    activation=\"relu\",  # 활성화 함수\n",
    "    cnn_channels=[128, 64]  # CNN 레이어의 채널 수\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "    num_epochs=num_epochs, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4937b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T04:17:59.882380Z",
     "start_time": "2025-04-02T04:17:59.882367Z"
    },
    "id": "af4937b4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(1, num_epochs+1), train_losses, 'b-', label='Training Loss')\n",
    "plt.plot(range(1, num_epochs+1), val_losses, 'r-', label='Validation Loss')\n",
    "plt.axvline(x=best_epoch+1, color='g', linestyle='--', label=f'Best Epoch ({best_epoch+1})')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curves')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gUKYgHkriIvf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T04:17:59.884225Z",
     "start_time": "2025-04-02T04:17:59.884212Z"
    },
    "id": "gUKYgHkriIvf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def evaluate_model(model, test_loader, device, scaler_y):\n",
    "    model.eval()\n",
    "    y_true_list = []\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred = model(x)\n",
    "\n",
    "            y_true_list.append(y.cpu().numpy())\n",
    "            y_pred_list.append(y_pred.cpu().numpy())\n",
    "\n",
    "    y_true = scaler_y.inverse_transform(np.concatenate(y_true_list))\n",
    "    y_pred = scaler_y.inverse_transform(np.concatenate(y_pred_list))\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    print(f\"MSE (Mean Squared Error): {mse:.4f}\")\n",
    "    print(f\"MAE (Mean Absolute Error): {mae:.4f}\")\n",
    "    print(f\"RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "def visualize_predictions(y_true, y_pred, num_samples=10):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_true[:num_samples, 0], y_pred[:num_samples, 0], color='blue', alpha=0.7)\n",
    "    plt.plot([y_true[:num_samples, 0].min(), y_true[:num_samples, 0].max()],\n",
    "             [y_true[:num_samples, 0].min(), y_true[:num_samples, 0].max()],\n",
    "             'r--', lw=2)\n",
    "    plt.title('Systolic Blood Pressure Prediction')\n",
    "    plt.xlabel('True Systolic BP')\n",
    "    plt.ylabel('Predicted Systolic BP')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_true[:num_samples, 1], y_pred[:num_samples, 1], color='green', alpha=0.7)\n",
    "    plt.plot([y_true[:num_samples, 1].min(), y_true[:num_samples, 1].max()],\n",
    "             [y_true[:num_samples, 1].min(), y_true[:num_samples, 1].max()],\n",
    "             'r--', lw=2)\n",
    "    plt.title('Diastolic Blood Pressure Prediction')\n",
    "    plt.xlabel('True Diastolic BP')\n",
    "    plt.ylabel('Predicted Diastolic BP')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_error_distribution(y_true, y_pred):\n",
    "    errors_systolic = y_true[:, 0] - y_pred[:, 0]\n",
    "    errors_diastolic = y_true[:, 1] - y_pred[:, 1]\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(errors_systolic, bins=30, color='blue', alpha=0.7)\n",
    "    plt.title('Error Distribution - Systolic BP')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(errors_diastolic, bins=30, color='green', alpha=0.7)\n",
    "    plt.title('Error Distribution - Diastolic BP')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "y_true, y_pred = evaluate_model(model, test_loader, device, scaler_y)\n",
    "visualize_predictions(y_true, y_pred)\n",
    "visualize_error_distribution(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ELtmAyzriZlD",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T04:17:59.885511Z",
     "start_time": "2025-04-02T04:17:59.885498Z"
    },
    "id": "ELtmAyzriZlD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def visualize_bp_predictions(y_true, y_pred, num_samples=100, title=\"PPG-BP Prediction Results\"):\n",
    "    \"\"\"\n",
    "    혈압 예측 결과를 시각화하는 함수\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : numpy.ndarray\n",
    "        실제 혈압 값 (SBP, DBP) 배열, 형태 [n_samples, 2]\n",
    "    y_pred : numpy.ndarray\n",
    "        예측된 혈압 값 (SBP, DBP) 배열, 형태 [n_samples, 2]\n",
    "    num_samples : int, optional (default=100)\n",
    "        시각화할 샘플의 수\n",
    "    title : str, optional\n",
    "        그래프 제목\n",
    "    \"\"\"\n",
    "    # 지정된 샘플 수만큼 데이터 선택\n",
    "    y_true_subset = y_true[:num_samples]\n",
    "    y_pred_subset = y_pred[:num_samples]\n",
    "\n",
    "    # RMSE와 MAE 계산\n",
    "    mse = mean_squared_error(y_true_subset, y_pred_subset)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true_subset, y_pred_subset)\n",
    "\n",
    "    # 그래프 크기 설정\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # 샘플 인덱스 (x축)\n",
    "    x = np.arange(len(y_true_subset))\n",
    "\n",
    "    # 수축기 혈압(SBP)과 이완기 혈압(DBP) 플롯\n",
    "    plt.plot(x, y_true_subset[:, 0], 'b-', label='True SBP')\n",
    "    plt.plot(x, y_pred_subset[:, 0], 'orange', label='Predicted SBP')\n",
    "    plt.plot(x, y_true_subset[:, 1], 'g-', label='True DBP')\n",
    "    plt.plot(x, y_pred_subset[:, 1], 'r-', label='Predicted DBP')\n",
    "\n",
    "    # 그래프 설정\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Blood Pressure (mmHg)')\n",
    "    plt.title(f'{title} (First {num_samples} Samples)')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(False)\n",
    "\n",
    "    # RMSE와 MAE 표시\n",
    "    plt.figtext(0.02, 0.98, f'✅ Evaluation Result | RMSE: {rmse:.2f} | MAE: {mae:.2f}',\n",
    "                fontsize=12, color='green', va='top')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 함수 사용 예시:\n",
    "\"\"\"\n",
    "# 평가 함수로부터 y_true와 y_pred를 받은 후:\n",
    "y_true, y_pred = evaluate_model(model, test_loader, device, scaler_y)\n",
    "\n",
    "# 시각화 함수 호출\n",
    "visualize_bp_predictions(y_true, y_pred, num_samples=100, title=\"PPG-Deformer Prediction\")\n",
    "\"\"\"\n",
    "\n",
    "def visualize_bp_scatter(y_true, y_pred, title=\"Blood Pressure Prediction Scatter Plot\"):\n",
    "    \"\"\"\n",
    "    혈압 예측 결과의 산점도를 그리는 함수\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : numpy.ndarray\n",
    "        실제 혈압 값 (SBP, DBP) 배열, 형태 [n_samples, 2]\n",
    "    y_pred : numpy.ndarray\n",
    "        예측된 혈압 값 (SBP, DBP) 배열, 형태 [n_samples, 2]\n",
    "    title : str, optional\n",
    "        그래프 제목\n",
    "    \"\"\"\n",
    "    # RMSE와 MAE 계산\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    # 그래프 설정\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # 수축기 혈압(SBP) 산점도\n",
    "    ax1.scatter(y_true[:, 0], y_pred[:, 0], alpha=0.5, color='blue')\n",
    "\n",
    "    # 이상적인 예측선 (y=x)\n",
    "    min_sbp = min(np.min(y_true[:, 0]), np.min(y_pred[:, 0]))\n",
    "    max_sbp = max(np.max(y_true[:, 0]), np.max(y_pred[:, 0]))\n",
    "    ax1.plot([min_sbp, max_sbp], [min_sbp, max_sbp], 'r--')\n",
    "\n",
    "    ax1.set_title('Systolic BP Prediction')\n",
    "    ax1.set_xlabel('True SBP (mmHg)')\n",
    "    ax1.set_ylabel('Predicted SBP (mmHg)')\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 이완기 혈압(DBP) 산점도\n",
    "    ax2.scatter(y_true[:, 1], y_pred[:, 1], alpha=0.5, color='green')\n",
    "\n",
    "    # 이상적인 예측선 (y=x)\n",
    "    min_dbp = min(np.min(y_true[:, 1]), np.min(y_pred[:, 1]))\n",
    "    max_dbp = max(np.max(y_true[:, 1]), np.max(y_pred[:, 1]))\n",
    "    ax2.plot([min_dbp, max_dbp], [min_dbp, max_dbp], 'r--')\n",
    "\n",
    "    ax2.set_title('Diastolic BP Prediction')\n",
    "    ax2.set_xlabel('True DBP (mmHg)')\n",
    "    ax2.set_ylabel('Predicted DBP (mmHg)')\n",
    "    ax2.set_aspect('equal')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # RMSE와 MAE 표시\n",
    "    plt.figtext(0.02, 0.98, f'✅ Evaluation Results | RMSE: {rmse:.2f} | MAE: {mae:.2f}',\n",
    "                fontsize=12, color='green', va='top')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.88)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_error_distribution(y_true, y_pred, bins=30):\n",
    "    \"\"\"\n",
    "    예측 오차의 분포를 히스토그램으로 시각화하는 함수\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : numpy.ndarray\n",
    "        실제 혈압 값 (SBP, DBP) 배열, 형태 [n_samples, 2]\n",
    "    y_pred : numpy.ndarray\n",
    "        예측된 혈압 값 (SBP, DBP) 배열, 형태 [n_samples, 2]\n",
    "    bins : int, optional\n",
    "        히스토그램의 bin 수\n",
    "    \"\"\"\n",
    "    # 오차 계산\n",
    "    errors_sbp = y_true[:, 0] - y_pred[:, 0]\n",
    "    errors_dbp = y_true[:, 1] - y_pred[:, 1]\n",
    "\n",
    "    # 그래프 설정\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # 수축기 혈압(SBP) 오차 히스토그램\n",
    "    ax1.hist(errors_sbp, bins=bins, alpha=0.7, color='blue')\n",
    "    ax1.axvline(x=0, color='r', linestyle='--')\n",
    "    ax1.set_title('SBP Error Distribution')\n",
    "    ax1.set_xlabel('Error (mmHg)')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "\n",
    "    # 통계값 계산 및 표시\n",
    "    mean_error_sbp = np.mean(errors_sbp)\n",
    "    std_error_sbp = np.std(errors_sbp)\n",
    "    ax1.text(0.05, 0.95, f'Mean: {mean_error_sbp:.2f}\\nStd: {std_error_sbp:.2f}',\n",
    "             transform=ax1.transAxes, va='top')\n",
    "\n",
    "    # 이완기 혈압(DBP) 오차 히스토그램\n",
    "    ax2.hist(errors_dbp, bins=bins, alpha=0.7, color='green')\n",
    "    ax2.axvline(x=0, color='r', linestyle='--')\n",
    "    ax2.set_title('DBP Error Distribution')\n",
    "    ax2.set_xlabel('Error (mmHg)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "\n",
    "    # 통계값 계산 및 표시\n",
    "    mean_error_dbp = np.mean(errors_dbp)\n",
    "    std_error_dbp = np.std(errors_dbp)\n",
    "    ax2.text(0.05, 0.95, f'Mean: {mean_error_dbp:.2f}\\nStd: {std_error_dbp:.2f}',\n",
    "             transform=ax2.transAxes, va='top')\n",
    "\n",
    "    plt.suptitle('Prediction Error Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 복합 시각화 함수 (위 세 가지 시각화를 모두 수행)\n",
    "def comprehensive_bp_visualization(y_true, y_pred, num_samples=100):\n",
    "    \"\"\"\n",
    "    혈압 예측 결과에 대한 종합적인 시각화를 수행하는 함수\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : numpy.ndarray\n",
    "        실제 혈압 값 (SBP, DBP) 배열, 형태 [n_samples, 2]\n",
    "    y_pred : numpy.ndarray\n",
    "        예측된 혈압 값 (SBP, DBP) 배열, 형태 [n_samples, 2]\n",
    "    num_samples : int, optional\n",
    "        시계열 시각화에 사용할 샘플 수\n",
    "    \"\"\"\n",
    "    # 시계열 시각화\n",
    "    visualize_bp_predictions(y_true, y_pred, num_samples, title=\"PPG-BP Prediction\")\n",
    "\n",
    "    # 산점도 시각화\n",
    "    visualize_bp_scatter(y_true, y_pred)\n",
    "\n",
    "    # 오차 분포 시각화\n",
    "    visualize_error_distribution(y_true, y_pred)\n",
    "\n",
    "# 모델 평가 후 실제값(y_true)과 예측값(y_pred) 얻기\n",
    "y_true, y_pred = evaluate_model(model, test_loader, device, scaler_y)\n",
    "\n",
    "# 이미지에서 보이는 것과 같은 시각화 생성\n",
    "visualize_bp_predictions(y_true, y_pred, num_samples=100, title=\"PPG-Deformer Prediction\")\n",
    "\n",
    "# 혹은 모든 시각화를 한번에 수행하려면:\n",
    "# comprehensive_bp_visualization(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oNHggMsjZAVo",
   "metadata": {
    "id": "oNHggMsjZAVo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
