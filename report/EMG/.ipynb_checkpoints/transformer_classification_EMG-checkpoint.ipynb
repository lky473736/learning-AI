{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b6ae38",
   "metadata": {},
   "source": [
    "## learning-AI : EMG classification\n",
    "### EMG Physical Action Data Set을 transformer를 통한 classification\n",
    "\n",
    "<br>\n",
    "\n",
    "- **임규연 (lky473736)**\n",
    "- 2024.09.17.에 문서 작성\n",
    "- **dataset** : https://archive.ics.uci.edu/dataset/213/emg+physical+action+data+set\n",
    "- **data abstract** : The Physical Action Data Set includes 10 normal and 10 aggressive physical actions that measure the human activity. The data have been collected by 4 subjects using the Delsys EMG wireless apparatus.\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "\n",
    "## <span id='dl'><mark>DL</mark></span>\n",
    "    \n",
    "EMG Physical Action Data Set을 classification한다. transformer을 이용한다.\n",
    "\n",
    "- **Reference**\n",
    "    - https://archive.ics.uci.edu/dataset/213/emg+physical+action+data+set\n",
    "    - https://www.ucihealth.org/health-library/content?contentTypeID=92&contentID=P07656&language=en\n",
    "    - https://www.kaggle.com/code/durgancegaur/emg-dataset\n",
    "    - https://www.kaggle.com/code/rachit2702/notebook6db9079b5a\n",
    "    - https://ieeexplore.ieee.org/document/10288050/references#references\n",
    "    - https://medium.com/analytics-vidhya/analysis-of-emg-physical-data-aggressive-normal-activities-4d5a696730b4\n",
    "    - https://keras.io/examples/timeseries/timeseries_classification_transformer/\n",
    "    - https://github.com/MyungKyuYi/HAR/blob/main/Transformer_WISDM_1204.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e36238a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.101Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-18 11:03:18.090702: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Dropout, Reshape, Conv1D, BatchNormalization, Activation, AveragePooling1D, GlobalAveragePooling1D, Lambda, Input, Concatenate, Add, UpSampling1D, Multiply\n",
    "from keras.models import Model\n",
    "# objectives 작동 X -> losses로 변경\n",
    "from keras.losses import mean_squared_error\n",
    "from keras import backend as K\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau,LearningRateScheduler\n",
    "from keras.initializers import random_normal\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc83cc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.102Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "\n",
    "import glob\n",
    "\n",
    "# data dir\n",
    "directory = '../../data/EMG/EMG'\n",
    "\n",
    "behaviors = [\n",
    "    # Aggressive\n",
    "    \"Elbowing\",\n",
    "    \"Frontkicking\",\n",
    "    \"Hamering\",\n",
    "    \"Headering\",\n",
    "    \"Kneeing\",\n",
    "    \"Pulling\",\n",
    "    \"Punching\",\n",
    "    \"Pushing\",\n",
    "    \"Sidekicking\",\n",
    "    \"Slapping\",\n",
    "    \n",
    "    # Normal\n",
    "    \"Bowing\",\n",
    "    \"Clapping\",\n",
    "    \"Handshaking\",\n",
    "    \"Hugging\",\n",
    "    \"Jumping\",\n",
    "    \"Running\",\n",
    "    \"Seating\",\n",
    "    \"Standing\",\n",
    "    \"Walking\",\n",
    "    \"Waving\"\n",
    "]\n",
    "\n",
    "# dataframe 넣을 directory\n",
    "# 이중 directory 구조, 각 행동마다에 dataframe 넣을 것\n",
    "dir_df = {}\n",
    "for i in range (1, 5) :\n",
    "    dir_compo = {}\n",
    "    for behavior in behaviors : # 행동\n",
    "        dir_compo[behavior] = None\n",
    "    dir_df[f'sub{i}'] = dir_compo\n",
    "    \n",
    "print (dir_df)\n",
    "\n",
    "txt_files = glob.glob(os.path.join(directory, '**', '*.txt'), \n",
    "                      recursive=True) # 하위 디렉토리까지 txt 파일 찾기\n",
    "\n",
    "# readme_file = os.path.join(directory, 'readme.txt')\n",
    "# txt_files = [file for file in txt_files if file != readme_file]\n",
    "\n",
    "# header (feature의 이름들)\n",
    "muscles = [\n",
    "    \"R-Bic\",\n",
    "    \"R-Tri\",\n",
    "    \"R-Thi\",\n",
    "    \"R-Ham\",\n",
    "    \"L-Bic\",\n",
    "    \"L-Tri\",\n",
    "    \"L-Thi\",\n",
    "    \"L-Ham\"\n",
    "]\n",
    "\n",
    "# txt -> csv\n",
    "for i, txt_file in enumerate(txt_files) :\n",
    "    try :\n",
    "        with open(txt_file, 'r') as file :\n",
    "            lines = file.readlines() # 파일 읽기\n",
    "\n",
    "        csv_file = txt_file.replace('.txt', '.csv')\n",
    "\n",
    "        with open(csv_file, 'w', encoding='utf-8') as file :\n",
    "            for line in lines : # 라인마다\n",
    "                csv_line = line.strip().replace('\\t', ',')\n",
    "                file.write(csv_line + '\\n')\n",
    "\n",
    "        os.remove(txt_file) # 원본 삭제\n",
    "        \n",
    "        print(f'{txt_file} -> {csv_file} : SUCCESS')\n",
    "    \n",
    "        df_temp = pd.read_csv(csv_file, names=muscles)\n",
    "        print (df_temp.shape)\n",
    "        \n",
    "        # 피실험자 번호 찾기 -> directory에 데이터프레임 넣기\n",
    "        for j in range(1, 5) :\n",
    "            if str(j) in csv_file :\n",
    "                for behavior in behaviors :\n",
    "                    if behavior in csv_file :\n",
    "                        dir_df[f'sub{j}'][behavior] = df_temp\n",
    "                        break\n",
    "        \n",
    "    except Exception as e :\n",
    "        print(f\"{e}\")\n",
    "        \n",
    "    print ('\\n---------------------\\n')\n",
    "\n",
    "print (dir_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c600fe",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.104Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 모든 파일을 열어서 target 열 추가\n",
    "\n",
    "behavior_to_index = {\n",
    "    \"Elbowing\": 0,\n",
    "    \"Frontkicking\": 1,\n",
    "    \"Hamering\": 2,\n",
    "    \"Headering\": 3,\n",
    "    \"Kneeing\": 4,\n",
    "    \"Pulling\": 5,\n",
    "    \"Punching\": 6,\n",
    "    \"Pushing\": 7,\n",
    "    \"Sidekicking\": 8,\n",
    "    \"Slapping\": 9,\n",
    "    \"Bowing\": 10,\n",
    "    \"Clapping\": 11,\n",
    "    \"Handshaking\": 12,\n",
    "    \"Hugging\": 13,\n",
    "    \"Jumping\": 14,\n",
    "    \"Running\": 15,\n",
    "    \"Seating\": 16,\n",
    "    \"Standing\": 17,\n",
    "    \"Walking\": 18,\n",
    "    \"Waving\": 19\n",
    "}\n",
    "\n",
    "\n",
    "for key_1 in dir_df.keys() : \n",
    "    for key_2 in dir_df[key_1].keys() : \n",
    "        dir_df[key_1][key_2]['target'] = behavior_to_index[key_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb489f26",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.105Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모든 파일의 결측치 전처리 (결측치를 평균값으로 대체)\n",
    "\n",
    "for key_1 in dir_df.keys() : \n",
    "    for key_2 in dir_df[key_1].keys() : \n",
    "        print (dir_df[key_1][key_2].isnull().sum())\n",
    "        dir_df[key_1][key_2] = dir_df[key_1][key_2].fillna(dir_df[key_1][key_2].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d815c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.107Z"
    }
   },
   "outputs": [],
   "source": [
    "# normal set, aggressive set prepare\n",
    "\n",
    "aggressive_features = behaviors[0:10]\n",
    "normal_features = behaviors[10:]\n",
    "\n",
    "print (\"normal_features : \", normal_features)\n",
    "print (\"aggressive_features : \", aggressive_features)\n",
    "\n",
    "########\n",
    "\n",
    "normal_set_sub1 = pd.concat([dir_df['sub1'][key] for key in normal_features])\n",
    "print (normal_set_sub1.info())\n",
    "print (normal_set_sub1.shape)\n",
    "\n",
    "aggressive_set_sub1 = pd.concat([dir_df['sub1'][key] for key in aggressive_features])\n",
    "print (aggressive_set_sub1.info())\n",
    "print (aggressive_set_sub1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60365649",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.108Z"
    }
   },
   "outputs": [],
   "source": [
    "# df 준비\n",
    "\n",
    "df = pd.concat([normal_set_sub1, aggressive_set_sub1])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b729a30",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.109Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    oversampling은 함수로 구현한다. 원래는 SMOTE를 사용하려고 하였다.\n",
    "    SMOTE (Synthetic Minority Over-sampling Technique)는 적은 수의 \n",
    "    클래스 사이에서 새로운 가상 records를 구성하는 것이다.\n",
    "    하지만, 랜덤으로 위치를 다시 uniting하기 때문에 시계열 데이터에는 적합하지 않아 직접 구현한다.\n",
    "    \n",
    "    여기서는 이 함수를 각 target마다의 record 수를 맞추어주기 위함으로 사용한다.\n",
    "'''\n",
    "\n",
    "def oversampling(df, target_col, max_size) :\n",
    "    # 결과를 저장할 리스트 \n",
    "    dfs = []\n",
    "    \n",
    "    for label in df[target_col].unique() :\n",
    "        class_df = df[df[target_col] == label]\n",
    "        \n",
    "        if len(class_df) < max_size :\n",
    "            # 샘플 수가 max_size보다 적으면 데이터를 복제하여 max_size로 만듦\n",
    "            sampled_df = class_df.sample(max_size, replace=True, random_state=42)\n",
    "        else :\n",
    "            # 샘플 수가 max_size보다 많으면 앞부분부터 max_size만큼 선택함\n",
    "            sampled_df = class_df.head(max_size)\n",
    "        \n",
    "        # 리스트에 추가\n",
    "        dfs.append(sampled_df)\n",
    "    \n",
    "    # 리스트에 저장된 데이터프레임들을 합침\n",
    "    df_resampled = pd.concat(dfs).reset_index(drop=True)\n",
    "    \n",
    "    return df_resampled\n",
    "\n",
    "df_resampled = oversampling(df, 'target', max_size=15000)\n",
    "print (df_resampled['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158f9a7a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.111Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the input, target\n",
    "\n",
    "EMG_input = df_resampled.drop(columns=['target'])\n",
    "EMG_target = df_resampled['target']\n",
    "\n",
    "print (EMG_input.head())\n",
    "print (EMG_target.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49176cf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.113Z"
    }
   },
   "outputs": [],
   "source": [
    "# z-score normalization 수행\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "EMG_input_scaled = scaler.fit_transform(EMG_input)\n",
    "EMG_target_reshaped = EMG_target.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# normalization된 입력 데이터와 label 데이터를 결합하여 DataFrame 생성\n",
    "df = pd.DataFrame(\n",
    "    np.hstack((EMG_input_scaled, EMG_target_reshaped)),\n",
    "    columns=[feature for feature in muscles] + ['target']\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca27ea",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.115Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    시계열 데이터를 프레임 크기와 홉 크기로 분할\n",
    "    일종의 split_sequence과 비슷하다고 보임\n",
    "    \n",
    "    아래 함수는 현재 x, y, z를 time-frame으로 split하고, hop_size가 일종의 이동량의 역할을 한다.\n",
    "    최종적으로는 frames와 labels를 반환하고 있다.\n",
    "    \n",
    "    아래를 현재 EMG 데이터셋에 맞게끔 변형할 것이다. 다음 셀을 참고.\n",
    "'''\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "def get_frames(df, frame_size, hop_size):\n",
    "\n",
    "    N_FEATURES = 3\n",
    "\n",
    "    frames = []\n",
    "    labels = []\n",
    "    for i in range(0, len(df) - frame_size, hop_size):\n",
    "        x = df['x'].values[i: i + frame_size]\n",
    "        y = df['y'].values[i: i + frame_size]\n",
    "        z = df['z'].values[i: i + frame_size]\n",
    "        \n",
    "        # Retrieve the most often used label in this segment\n",
    "        label = stats.mode(df['label'][i: i + frame_size])[0][0]\n",
    "        frames.append([x, y, z])\n",
    "        labels.append(label)\n",
    "\n",
    "    # Bring the segments into a better shape\n",
    "    frames = np.asarray(frames).reshape(-1, frame_size, N_FEATURES)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    return frames, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7b835",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.116Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_frames(df, frame_size, hop_size):\n",
    "#     N_FEATURES = 6  # 6개의 특징 (back_x, back_y, back_z, thigh_x, thigh_y, thigh_z)\n",
    "\n",
    "#     frames = []\n",
    "#     labels = []\n",
    "#     for i in range(0, len(df) - frame_size, hop_size):\n",
    "#         # 각 축의 데이터를 슬라이싱\n",
    "#         back_x = df['back_x'].values[i: i + frame_size]\n",
    "#         back_y = df['back_y'].values[i: i + frame_size]\n",
    "#         back_z = df['back_z'].values[i: i + frame_size]\n",
    "#         thigh_x = df['thigh_x'].values[i: i + frame_size]\n",
    "#         thigh_y = df['thigh_y'].values[i: i + frame_size]\n",
    "#         thigh_z = df['thigh_z'].values[i: i + frame_size]\n",
    "\n",
    "#         # 해당 구간의 레이블에서 가장 많이 나타나는 레이블 선택\n",
    "#         mode_result = stats.mode(df['label'][i: i + frame_size])\n",
    "#         label = mode_result.mode[0]  # mode_result.mode는 배열이므로 첫 번째 값을 선택\n",
    "\n",
    "#         # 각 축의 데이터를 하나의 리스트로 묶어 프레임에 추가\n",
    "#         frames.append([back_x, back_y, back_z, thigh_x, thigh_y, thigh_z])\n",
    "#         labels.append(label)\n",
    "\n",
    "#     # 프레임을 numpy 배열로 변환하고, (number_of_frames, frame_size, N_FEATURES) 형태로 reshape\n",
    "#     frames = np.asarray(frames).reshape(-1, frame_size, N_FEATURES)\n",
    "#     labels = np.asarray(labels)\n",
    "\n",
    "#     return frames, labels\n",
    "\n",
    "def get_frames(df, frame_size, hop_size) :\n",
    "    N_FEATURES = 8  # attribute 8개\n",
    "    frames = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(0, len(df) - frame_size + 1, hop_size) :\n",
    "        r_bic = df['R-Bic'].values[i: i + frame_size]\n",
    "        r_tri = df['R-Tri'].values[i: i + frame_size]\n",
    "        r_thi = df['R-Thi'].values[i: i + frame_size]\n",
    "        r_ham = df['R-Ham'].values[i: i + frame_size]\n",
    "        l_bic = df['L-Bic'].values[i: i + frame_size]\n",
    "        l_tri = df['L-Tri'].values[i: i + frame_size]\n",
    "        l_thi = df['L-Thi'].values[i: i + frame_size]\n",
    "        l_ham = df['L-Ham'].values[i: i + frame_size]\n",
    "\n",
    "        mode_result = stats.mode(df['target'][i: i + frame_size], keepdims=False) # 가장 많이 빈출하는 값 찾기\n",
    "        \n",
    "        # 디버깅 정보 출력\n",
    "        print (f\"Mode Result: {mode_result}\")\n",
    "\n",
    "        '''\n",
    "            여기서 mode_result가 scala값이라서 오류가 발생하는 경우가 생김\n",
    "            따라서 여기서 isinstance로 타입 확인하고 그에 따라서 대처함\n",
    "        '''\n",
    "        if isinstance(mode_result.mode, np.ndarray) : # 만약 mode 결과가 스칼라인 경우 처리\n",
    "            if mode_result.mode.size > 0 :\n",
    "                label = mode_result.mode[0]\n",
    "            else:\n",
    "                label = -1\n",
    "        else :\n",
    "            label = mode_result.mode  # 스칼라일 경우 직접 사용\n",
    "\n",
    "        # 각 축을 묶어서 데이터 추가함\n",
    "        frames.append([r_bic, r_tri, r_thi, r_ham, l_bic, l_tri, l_thi, l_ham])\n",
    "        labels.append(label)\n",
    "\n",
    "    frames = np.asarray(frames).reshape(-1, frame_size, N_FEATURES) # reshape\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    return frames, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4080a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.118Z"
    }
   },
   "outputs": [],
   "source": [
    "# input, target split\n",
    "\n",
    "Fs = 100\n",
    "frame_size = Fs * 4\n",
    "hop_size = Fs * 2\n",
    "\n",
    "EMG_input, EMG_target = get_frames(df, frame_size, hop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9cb505",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.119Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(EMG_input, EMG_target, \n",
    "                                                    test_size = 0.1,\n",
    "                                                    stratify = EMG_target)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9497dbf9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.120Z"
    }
   },
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = keras.layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = keras.layers.Dropout(dropout)(x)\n",
    "    x = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
    "    x = keras.layers.Dropout(dropout)(x)\n",
    "    x = keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287f039",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.121Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The main part of our model is now complete. We can stack multiple of those\n",
    "    `transformer_encoder` blocks and we can also proceed to add the final\n",
    "    Multi-Layer Perceptron classification head. Apart from a stack of `Dense`\n",
    "    layers, we need to reduce the output tensor of the `TransformerEncoder` part of\n",
    "    our model down to a vector of features for each data point in the current\n",
    "    batch. A common way to achieve this is to use a pooling layer. For\n",
    "    this example, a `GlobalAveragePooling1D` layer is sufficient.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = keras.layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = keras.layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = keras.layers.Dense(len(df['target'].unique()), activation=\"softmax\")(x) # label\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d5d14",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.124Z"
    }
   },
   "outputs": [],
   "source": [
    "# modelling\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "\n",
    "input_shape = (frame_size, 8)\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "keras.utils.plot_model(model, \n",
    "                      show_dtype=True,\n",
    "                      show_layer_activations=True,\n",
    "                      show_layer_names=True,\n",
    "                      show_shapes=True,\n",
    "                      show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1d91fb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.126Z"
    }
   },
   "outputs": [],
   "source": [
    "# compile\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    #loss=\"categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    #metrics=[\"sparse_categorical_accuracy\"],\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf3db10",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.129Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit \n",
    "# memory 문제가 일어날 수 있기 때문에 memory_profiler 도입\n",
    "\n",
    "from memory_profiler import profile\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
    "\n",
    "@profile # 메모리 오버플로우로 인한 시스템 다운을 막기 위해서\n",
    "def train_model() :\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "history = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a98d96b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.130Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "\n",
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eced24",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.131Z"
    }
   },
   "outputs": [],
   "source": [
    "# history 그리고 loss 시각화\n",
    "\n",
    "print (history.history)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('number of epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend (['train loss', 'validation loss'])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e8ed5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.132Z"
    }
   },
   "outputs": [],
   "source": [
    "# acc 시각화\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.xlabel('number of epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend (['train acc', 'validation acc'])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed4fc3b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.133Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "y_test_pred_classes = np.argmax(y_test_pred, axis=1)\n",
    "\n",
    "############\n",
    "\n",
    "cm_test = confusion_matrix(y_test, y_test_pred_classes)\n",
    "print (\"- test data confusion matrix -\\n\")\n",
    "print (cm_test)\n",
    "\n",
    "print ('\\n --------------- \\n')\n",
    "\n",
    "cr_test = classification_report(y_test, y_test_pred_classes)\n",
    "print (\"- test data report of classification -\\n\")\n",
    "print (cr_test)\n",
    "\n",
    "print ('\\n --------------- \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687c7c50",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-18T02:02:55.134Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('confusion matrix - test set')\n",
    "plt.xlabel('predict label')\n",
    "plt.ylabel('true')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
