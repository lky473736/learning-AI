{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 04:36:17.696811: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-22 04:36:17.697012: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-22 04:36:17.718212: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-22 04:36:17.759327: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "import os\n",
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"/workspace/rtu_data_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33696013 entries, 0 to 33696012\n",
      "Data columns (total 19 columns):\n",
      " #   Column                Dtype  \n",
      "---  ------                -----  \n",
      " 0   module(equipment)     object \n",
      " 1   timestamp             int64  \n",
      " 2   localtime             int64  \n",
      " 3   operation             int64  \n",
      " 4   voltageR              float64\n",
      " 5   voltageS              float64\n",
      " 6   voltageT              float64\n",
      " 7   voltageRS             float64\n",
      " 8   voltageST             float64\n",
      " 9   voltageTR             float64\n",
      " 10  currentR              float64\n",
      " 11  currentS              float64\n",
      " 12  currentT              float64\n",
      " 13  activePower           float64\n",
      " 14  powerFactorR          float64\n",
      " 15  powerFactorS          float64\n",
      " 16  powerFactorT          float64\n",
      " 17  reactivePowerLagging  float64\n",
      " 18  accumActiveEnergy     int64  \n",
      "dtypes: float64(14), int64(4), object(1)\n",
      "memory usage: 4.8+ GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>1.739519e+12</td>\n",
       "      <td>3.739821e+09</td>\n",
       "      <td>1.733040e+12</td>\n",
       "      <td>1.736280e+12</td>\n",
       "      <td>1.739520e+12</td>\n",
       "      <td>1.742756e+12</td>\n",
       "      <td>1.745996e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>localtime</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>2.024839e+13</td>\n",
       "      <td>3.665090e+09</td>\n",
       "      <td>2.024120e+13</td>\n",
       "      <td>2.025011e+13</td>\n",
       "      <td>2.025021e+13</td>\n",
       "      <td>2.025032e+13</td>\n",
       "      <td>2.025043e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>operation</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voltageR</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>2.149938e+02</td>\n",
       "      <td>2.902114e+00</td>\n",
       "      <td>1.901000e+02</td>\n",
       "      <td>2.125000e+02</td>\n",
       "      <td>2.150000e+02</td>\n",
       "      <td>2.175000e+02</td>\n",
       "      <td>2.200000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voltageS</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>2.149947e+02</td>\n",
       "      <td>2.902552e+00</td>\n",
       "      <td>1.901400e+02</td>\n",
       "      <td>2.125000e+02</td>\n",
       "      <td>2.150000e+02</td>\n",
       "      <td>2.175000e+02</td>\n",
       "      <td>2.200000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voltageT</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>2.149943e+02</td>\n",
       "      <td>2.902357e+00</td>\n",
       "      <td>1.901800e+02</td>\n",
       "      <td>2.125000e+02</td>\n",
       "      <td>2.150000e+02</td>\n",
       "      <td>2.175000e+02</td>\n",
       "      <td>2.200000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voltageRS</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>3.723700e+02</td>\n",
       "      <td>3.573077e+00</td>\n",
       "      <td>3.308300e+02</td>\n",
       "      <td>3.698400e+02</td>\n",
       "      <td>3.723800e+02</td>\n",
       "      <td>3.749200e+02</td>\n",
       "      <td>3.810400e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voltageST</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>3.723705e+02</td>\n",
       "      <td>3.572666e+00</td>\n",
       "      <td>3.299500e+02</td>\n",
       "      <td>3.698400e+02</td>\n",
       "      <td>3.723800e+02</td>\n",
       "      <td>3.749100e+02</td>\n",
       "      <td>3.810400e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voltageTR</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>3.723697e+02</td>\n",
       "      <td>3.572804e+00</td>\n",
       "      <td>3.305600e+02</td>\n",
       "      <td>3.698400e+02</td>\n",
       "      <td>3.723800e+02</td>\n",
       "      <td>3.749200e+02</td>\n",
       "      <td>3.810400e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currentR</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>1.749940e+01</td>\n",
       "      <td>7.217217e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.125000e+01</td>\n",
       "      <td>1.750000e+01</td>\n",
       "      <td>2.375000e+01</td>\n",
       "      <td>3.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currentS</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>1.749925e+01</td>\n",
       "      <td>7.217180e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.125000e+01</td>\n",
       "      <td>1.750000e+01</td>\n",
       "      <td>2.375000e+01</td>\n",
       "      <td>3.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currentT</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>1.750193e+01</td>\n",
       "      <td>7.216743e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.125000e+01</td>\n",
       "      <td>1.750000e+01</td>\n",
       "      <td>2.375000e+01</td>\n",
       "      <td>3.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activePower</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>3.009952e+03</td>\n",
       "      <td>7.171454e+02</td>\n",
       "      <td>8.644400e+02</td>\n",
       "      <td>2.503820e+03</td>\n",
       "      <td>3.009850e+03</td>\n",
       "      <td>3.515890e+03</td>\n",
       "      <td>5.220930e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>powerFactorR</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>9.246882e+01</td>\n",
       "      <td>4.405299e+00</td>\n",
       "      <td>6.000000e+01</td>\n",
       "      <td>8.873000e+01</td>\n",
       "      <td>9.249000e+01</td>\n",
       "      <td>9.624000e+01</td>\n",
       "      <td>1.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>powerFactorS</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>9.247004e+01</td>\n",
       "      <td>4.405555e+00</td>\n",
       "      <td>6.000000e+01</td>\n",
       "      <td>8.873000e+01</td>\n",
       "      <td>9.249000e+01</td>\n",
       "      <td>9.625000e+01</td>\n",
       "      <td>1.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>powerFactorT</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>9.246884e+01</td>\n",
       "      <td>4.405619e+00</td>\n",
       "      <td>6.000000e+01</td>\n",
       "      <td>8.873000e+01</td>\n",
       "      <td>9.249000e+01</td>\n",
       "      <td>9.624000e+01</td>\n",
       "      <td>1.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reactivePowerLagging</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>6.019452e+02</td>\n",
       "      <td>2.290785e+02</td>\n",
       "      <td>8.944000e+01</td>\n",
       "      <td>4.231400e+02</td>\n",
       "      <td>5.739700e+02</td>\n",
       "      <td>7.554100e+02</td>\n",
       "      <td>1.550630e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accumActiveEnergy</th>\n",
       "      <td>33696013.0</td>\n",
       "      <td>8.103413e+06</td>\n",
       "      <td>3.344950e+06</td>\n",
       "      <td>1.129004e+06</td>\n",
       "      <td>5.393998e+06</td>\n",
       "      <td>8.103386e+06</td>\n",
       "      <td>1.081253e+07</td>\n",
       "      <td>1.543817e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           count          mean           std           min  \\\n",
       "timestamp             33696013.0  1.739519e+12  3.739821e+09  1.733040e+12   \n",
       "localtime             33696013.0  2.024839e+13  3.665090e+09  2.024120e+13   \n",
       "operation             33696013.0  1.000000e+00  0.000000e+00  1.000000e+00   \n",
       "voltageR              33696013.0  2.149938e+02  2.902114e+00  1.901000e+02   \n",
       "voltageS              33696013.0  2.149947e+02  2.902552e+00  1.901400e+02   \n",
       "voltageT              33696013.0  2.149943e+02  2.902357e+00  1.901800e+02   \n",
       "voltageRS             33696013.0  3.723700e+02  3.573077e+00  3.308300e+02   \n",
       "voltageST             33696013.0  3.723705e+02  3.572666e+00  3.299500e+02   \n",
       "voltageTR             33696013.0  3.723697e+02  3.572804e+00  3.305600e+02   \n",
       "currentR              33696013.0  1.749940e+01  7.217217e+00  5.000000e+00   \n",
       "currentS              33696013.0  1.749925e+01  7.217180e+00  5.000000e+00   \n",
       "currentT              33696013.0  1.750193e+01  7.216743e+00  5.000000e+00   \n",
       "activePower           33696013.0  3.009952e+03  7.171454e+02  8.644400e+02   \n",
       "powerFactorR          33696013.0  9.246882e+01  4.405299e+00  6.000000e+01   \n",
       "powerFactorS          33696013.0  9.247004e+01  4.405555e+00  6.000000e+01   \n",
       "powerFactorT          33696013.0  9.246884e+01  4.405619e+00  6.000000e+01   \n",
       "reactivePowerLagging  33696013.0  6.019452e+02  2.290785e+02  8.944000e+01   \n",
       "accumActiveEnergy     33696013.0  8.103413e+06  3.344950e+06  1.129004e+06   \n",
       "\n",
       "                               25%           50%           75%           max  \n",
       "timestamp             1.736280e+12  1.739520e+12  1.742756e+12  1.745996e+12  \n",
       "localtime             2.025011e+13  2.025021e+13  2.025032e+13  2.025043e+13  \n",
       "operation             1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n",
       "voltageR              2.125000e+02  2.150000e+02  2.175000e+02  2.200000e+02  \n",
       "voltageS              2.125000e+02  2.150000e+02  2.175000e+02  2.200000e+02  \n",
       "voltageT              2.125000e+02  2.150000e+02  2.175000e+02  2.200000e+02  \n",
       "voltageRS             3.698400e+02  3.723800e+02  3.749200e+02  3.810400e+02  \n",
       "voltageST             3.698400e+02  3.723800e+02  3.749100e+02  3.810400e+02  \n",
       "voltageTR             3.698400e+02  3.723800e+02  3.749200e+02  3.810400e+02  \n",
       "currentR              1.125000e+01  1.750000e+01  2.375000e+01  3.000000e+01  \n",
       "currentS              1.125000e+01  1.750000e+01  2.375000e+01  3.000000e+01  \n",
       "currentT              1.125000e+01  1.750000e+01  2.375000e+01  3.000000e+01  \n",
       "activePower           2.503820e+03  3.009850e+03  3.515890e+03  5.220930e+03  \n",
       "powerFactorR          8.873000e+01  9.249000e+01  9.624000e+01  1.000000e+02  \n",
       "powerFactorS          8.873000e+01  9.249000e+01  9.625000e+01  1.000000e+02  \n",
       "powerFactorT          8.873000e+01  9.249000e+01  9.624000e+01  1.000000e+02  \n",
       "reactivePowerLagging  4.231400e+02  5.739700e+02  7.554100e+02  1.550630e+03  \n",
       "accumActiveEnergy     5.393998e+06  8.103386e+06  1.081253e+07  1.543817e+07  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.info()\n",
    "df1.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module(equipment)       0\n",
      "timestamp               0\n",
      "localtime               0\n",
      "operation               0\n",
      "voltageR                0\n",
      "voltageS                0\n",
      "voltageT                0\n",
      "voltageRS               0\n",
      "voltageST               0\n",
      "voltageTR               0\n",
      "currentR                0\n",
      "currentS                0\n",
      "currentT                0\n",
      "activePower             0\n",
      "powerFactorR            0\n",
      "powerFactorS            0\n",
      "powerFactorT            0\n",
      "reactivePowerLagging    0\n",
      "accumActiveEnergy       0\n",
      "dtype: int64\n",
      "(33696013, 19)\n",
      "Index(['module(equipment)', 'timestamp', 'localtime', 'operation', 'voltageR',\n",
      "       'voltageS', 'voltageT', 'voltageRS', 'voltageST', 'voltageTR',\n",
      "       'currentR', 'currentS', 'currentT', 'activePower', 'powerFactorR',\n",
      "       'powerFactorS', 'powerFactorT', 'reactivePowerLagging',\n",
      "       'accumActiveEnergy'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print (df1.isnull().sum())\n",
    "print (df1.shape)\n",
    "print (df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'module(equipment)' ['1(PM-3)' '2(L-1전등)' '3(분쇄기(2))' '4(분쇄기(1))' '5(좌측분전반)' '11(우측분전반1)'\n",
      " '12(4호기)' '13(3호기)' '14(2호기)' '15(예비건조기)' '16(호이스트)' '17(6호기)'\n",
      " '18(우측분전반2)']\n",
      "'1(PM-3)': 2592001\n",
      "'2(L-1전등)': 2592001\n",
      "'3(분쇄기(2))': 2592001\n",
      "'4(분쇄기(1))': 2592001\n",
      "'5(좌측분전반)': 2592001\n",
      "'11(우측분전반1)': 2592001\n",
      "'12(4호기)': 2592001\n",
      "'13(3호기)': 2592001\n",
      "'14(2호기)': 2592001\n",
      "'15(예비건조기)': 2592001\n",
      "'16(호이스트)': 2592001\n",
      "'17(6호기)': 2592001\n",
      "'18(우측분전반2)': 2592001\n"
     ]
    }
   ],
   "source": [
    "column_name = 'module(equipment)'\n",
    "\n",
    "unique_values = df1[column_name].unique()\n",
    "print(f\"'{column_name}' {unique_values}\")\n",
    "\n",
    "for value in unique_values:\n",
    "    count = len(df1[df1[column_name] == value])\n",
    "    print(f\"'{value}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed = df1.set_index(['timestamp', 'localtime', 'module(equipment)'])\n",
    "df_wide = df_indexed.unstack('module(equipment)')\n",
    "\n",
    "df_wide.columns = [f\"{col[1][:col[1].index('(')]}_{col[0]}\" for col in df_wide.columns]\n",
    "df_wide = df_wide.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp\n",
      "localtime\n",
      "1_operation\n",
      "11_operation\n",
      "12_operation\n",
      "13_operation\n",
      "14_operation\n",
      "15_operation\n",
      "16_operation\n",
      "17_operation\n",
      "18_operation\n",
      "2_operation\n",
      "3_operation\n",
      "4_operation\n",
      "5_operation\n",
      "1_voltageR\n",
      "11_voltageR\n",
      "12_voltageR\n",
      "13_voltageR\n",
      "14_voltageR\n",
      "15_voltageR\n",
      "16_voltageR\n",
      "17_voltageR\n",
      "18_voltageR\n",
      "2_voltageR\n",
      "3_voltageR\n",
      "4_voltageR\n",
      "5_voltageR\n",
      "1_voltageS\n",
      "11_voltageS\n",
      "12_voltageS\n",
      "13_voltageS\n",
      "14_voltageS\n",
      "15_voltageS\n",
      "16_voltageS\n",
      "17_voltageS\n",
      "18_voltageS\n",
      "2_voltageS\n",
      "3_voltageS\n",
      "4_voltageS\n",
      "5_voltageS\n",
      "1_voltageT\n",
      "11_voltageT\n",
      "12_voltageT\n",
      "13_voltageT\n",
      "14_voltageT\n",
      "15_voltageT\n",
      "16_voltageT\n",
      "17_voltageT\n",
      "18_voltageT\n",
      "2_voltageT\n",
      "3_voltageT\n",
      "4_voltageT\n",
      "5_voltageT\n",
      "1_voltageRS\n",
      "11_voltageRS\n",
      "12_voltageRS\n",
      "13_voltageRS\n",
      "14_voltageRS\n",
      "15_voltageRS\n",
      "16_voltageRS\n",
      "17_voltageRS\n",
      "18_voltageRS\n",
      "2_voltageRS\n",
      "3_voltageRS\n",
      "4_voltageRS\n",
      "5_voltageRS\n",
      "1_voltageST\n",
      "11_voltageST\n",
      "12_voltageST\n",
      "13_voltageST\n",
      "14_voltageST\n",
      "15_voltageST\n",
      "16_voltageST\n",
      "17_voltageST\n",
      "18_voltageST\n",
      "2_voltageST\n",
      "3_voltageST\n",
      "4_voltageST\n",
      "5_voltageST\n",
      "1_voltageTR\n",
      "11_voltageTR\n",
      "12_voltageTR\n",
      "13_voltageTR\n",
      "14_voltageTR\n",
      "15_voltageTR\n",
      "16_voltageTR\n",
      "17_voltageTR\n",
      "18_voltageTR\n",
      "2_voltageTR\n",
      "3_voltageTR\n",
      "4_voltageTR\n",
      "5_voltageTR\n",
      "1_currentR\n",
      "11_currentR\n",
      "12_currentR\n",
      "13_currentR\n",
      "14_currentR\n",
      "15_currentR\n",
      "16_currentR\n",
      "17_currentR\n",
      "18_currentR\n",
      "2_currentR\n",
      "3_currentR\n",
      "4_currentR\n",
      "5_currentR\n",
      "1_currentS\n",
      "11_currentS\n",
      "12_currentS\n",
      "13_currentS\n",
      "14_currentS\n",
      "15_currentS\n",
      "16_currentS\n",
      "17_currentS\n",
      "18_currentS\n",
      "2_currentS\n",
      "3_currentS\n",
      "4_currentS\n",
      "5_currentS\n",
      "1_currentT\n",
      "11_currentT\n",
      "12_currentT\n",
      "13_currentT\n",
      "14_currentT\n",
      "15_currentT\n",
      "16_currentT\n",
      "17_currentT\n",
      "18_currentT\n",
      "2_currentT\n",
      "3_currentT\n",
      "4_currentT\n",
      "5_currentT\n",
      "1_activePower\n",
      "11_activePower\n",
      "12_activePower\n",
      "13_activePower\n",
      "14_activePower\n",
      "15_activePower\n",
      "16_activePower\n",
      "17_activePower\n",
      "18_activePower\n",
      "2_activePower\n",
      "3_activePower\n",
      "4_activePower\n",
      "5_activePower\n",
      "1_powerFactorR\n",
      "11_powerFactorR\n",
      "12_powerFactorR\n",
      "13_powerFactorR\n",
      "14_powerFactorR\n",
      "15_powerFactorR\n",
      "16_powerFactorR\n",
      "17_powerFactorR\n",
      "18_powerFactorR\n",
      "2_powerFactorR\n",
      "3_powerFactorR\n",
      "4_powerFactorR\n",
      "5_powerFactorR\n",
      "1_powerFactorS\n",
      "11_powerFactorS\n",
      "12_powerFactorS\n",
      "13_powerFactorS\n",
      "14_powerFactorS\n",
      "15_powerFactorS\n",
      "16_powerFactorS\n",
      "17_powerFactorS\n",
      "18_powerFactorS\n",
      "2_powerFactorS\n",
      "3_powerFactorS\n",
      "4_powerFactorS\n",
      "5_powerFactorS\n",
      "1_powerFactorT\n",
      "11_powerFactorT\n",
      "12_powerFactorT\n",
      "13_powerFactorT\n",
      "14_powerFactorT\n",
      "15_powerFactorT\n",
      "16_powerFactorT\n",
      "17_powerFactorT\n",
      "18_powerFactorT\n",
      "2_powerFactorT\n",
      "3_powerFactorT\n",
      "4_powerFactorT\n",
      "5_powerFactorT\n",
      "1_reactivePowerLagging\n",
      "11_reactivePowerLagging\n",
      "12_reactivePowerLagging\n",
      "13_reactivePowerLagging\n",
      "14_reactivePowerLagging\n",
      "15_reactivePowerLagging\n",
      "16_reactivePowerLagging\n",
      "17_reactivePowerLagging\n",
      "18_reactivePowerLagging\n",
      "2_reactivePowerLagging\n",
      "3_reactivePowerLagging\n",
      "4_reactivePowerLagging\n",
      "5_reactivePowerLagging\n",
      "1_accumActiveEnergy\n",
      "11_accumActiveEnergy\n",
      "12_accumActiveEnergy\n",
      "13_accumActiveEnergy\n",
      "14_accumActiveEnergy\n",
      "15_accumActiveEnergy\n",
      "16_accumActiveEnergy\n",
      "17_accumActiveEnergy\n",
      "18_accumActiveEnergy\n",
      "2_accumActiveEnergy\n",
      "3_accumActiveEnergy\n",
      "4_accumActiveEnergy\n",
      "5_accumActiveEnergy\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(df_wide.columns)) : \n",
    "    print (df_wide.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = []\n",
    "for equipment in ['1', '2', '3', '4', '5', '11', '12', '13', '14', '15', '16', '17', '18']:\n",
    "    important_features.extend([\n",
    "        f'{equipment}_activePower',\n",
    "        f'{equipment}_accumActiveEnergy', \n",
    "        f'{equipment}_operation',\n",
    "        f'{equipment}_voltageR',\n",
    "        f'{equipment}_voltageS', \n",
    "        f'{equipment}_voltageT',\n",
    "        f'{equipment}_currentR',\n",
    "        f'{equipment}_currentS',\n",
    "        f'{equipment}_currentT'\n",
    "    ])\n",
    "\n",
    "df_important = df_wide[['timestamp', 'localtime'] + important_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_important['total_activePower'] = sum([df_important[f'{equipment}_activePower'] for equipment in ['1', '2', '3', '4', '5', '11', '12', '13', '14', '15', '16', '17', '18']])\n",
    "df_important['total_operation'] = sum([df_important[f'{equipment}_operation'] for equipment in ['1', '2', '3', '4', '5', '11', '12', '13', '14', '15', '16', '17', '18']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_important['datetime'] = pd.to_datetime(df_important['timestamp'], unit='ms')\n",
    "df_important['hour'] = df_important['datetime'].dt.hour\n",
    "df_important['day'] = df_important['datetime'].dt.day\n",
    "df_important['weekday'] = df_important['datetime'].dt.weekday\n",
    "df_important['is_weekend'] = (df_important['weekday'] >= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>localtime</th>\n",
       "      <th>1_activePower</th>\n",
       "      <th>1_accumActiveEnergy</th>\n",
       "      <th>1_operation</th>\n",
       "      <th>1_voltageR</th>\n",
       "      <th>1_voltageS</th>\n",
       "      <th>1_voltageT</th>\n",
       "      <th>1_currentR</th>\n",
       "      <th>1_currentS</th>\n",
       "      <th>...</th>\n",
       "      <th>18_currentR</th>\n",
       "      <th>18_currentS</th>\n",
       "      <th>18_currentT</th>\n",
       "      <th>total_activePower</th>\n",
       "      <th>total_operation</th>\n",
       "      <th>datetime</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1733040000000</td>\n",
       "      <td>20241201000000</td>\n",
       "      <td>2961.61</td>\n",
       "      <td>1955004</td>\n",
       "      <td>1</td>\n",
       "      <td>214.38</td>\n",
       "      <td>214.45</td>\n",
       "      <td>219.10</td>\n",
       "      <td>15.16</td>\n",
       "      <td>15.53</td>\n",
       "      <td>...</td>\n",
       "      <td>18.70</td>\n",
       "      <td>9.71</td>\n",
       "      <td>25.52</td>\n",
       "      <td>35170.41</td>\n",
       "      <td>13</td>\n",
       "      <td>2024-12-01 08:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1733040005000</td>\n",
       "      <td>20241201000005</td>\n",
       "      <td>3017.48</td>\n",
       "      <td>1955008</td>\n",
       "      <td>1</td>\n",
       "      <td>214.05</td>\n",
       "      <td>211.74</td>\n",
       "      <td>218.68</td>\n",
       "      <td>25.70</td>\n",
       "      <td>7.07</td>\n",
       "      <td>...</td>\n",
       "      <td>11.18</td>\n",
       "      <td>29.47</td>\n",
       "      <td>13.38</td>\n",
       "      <td>40919.00</td>\n",
       "      <td>13</td>\n",
       "      <td>2024-12-01 08:00:05</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1733040010000</td>\n",
       "      <td>20241201000010</td>\n",
       "      <td>2408.01</td>\n",
       "      <td>1955011</td>\n",
       "      <td>1</td>\n",
       "      <td>215.79</td>\n",
       "      <td>214.92</td>\n",
       "      <td>211.10</td>\n",
       "      <td>13.64</td>\n",
       "      <td>14.87</td>\n",
       "      <td>...</td>\n",
       "      <td>23.18</td>\n",
       "      <td>26.97</td>\n",
       "      <td>24.94</td>\n",
       "      <td>40123.25</td>\n",
       "      <td>13</td>\n",
       "      <td>2024-12-01 08:00:10</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1733040015000</td>\n",
       "      <td>20241201000015</td>\n",
       "      <td>3289.33</td>\n",
       "      <td>1955016</td>\n",
       "      <td>1</td>\n",
       "      <td>210.39</td>\n",
       "      <td>214.92</td>\n",
       "      <td>215.57</td>\n",
       "      <td>25.76</td>\n",
       "      <td>26.35</td>\n",
       "      <td>...</td>\n",
       "      <td>28.25</td>\n",
       "      <td>19.21</td>\n",
       "      <td>6.53</td>\n",
       "      <td>40927.77</td>\n",
       "      <td>13</td>\n",
       "      <td>2024-12-01 08:00:15</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1733040020000</td>\n",
       "      <td>20241201000020</td>\n",
       "      <td>3069.31</td>\n",
       "      <td>1955020</td>\n",
       "      <td>1</td>\n",
       "      <td>216.71</td>\n",
       "      <td>216.37</td>\n",
       "      <td>215.65</td>\n",
       "      <td>8.65</td>\n",
       "      <td>29.49</td>\n",
       "      <td>...</td>\n",
       "      <td>9.19</td>\n",
       "      <td>19.51</td>\n",
       "      <td>16.25</td>\n",
       "      <td>38327.94</td>\n",
       "      <td>13</td>\n",
       "      <td>2024-12-01 08:00:20</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp       localtime  1_activePower  1_accumActiveEnergy  \\\n",
       "0  1733040000000  20241201000000        2961.61              1955004   \n",
       "1  1733040005000  20241201000005        3017.48              1955008   \n",
       "2  1733040010000  20241201000010        2408.01              1955011   \n",
       "3  1733040015000  20241201000015        3289.33              1955016   \n",
       "4  1733040020000  20241201000020        3069.31              1955020   \n",
       "\n",
       "   1_operation  1_voltageR  1_voltageS  1_voltageT  1_currentR  1_currentS  \\\n",
       "0            1      214.38      214.45      219.10       15.16       15.53   \n",
       "1            1      214.05      211.74      218.68       25.70        7.07   \n",
       "2            1      215.79      214.92      211.10       13.64       14.87   \n",
       "3            1      210.39      214.92      215.57       25.76       26.35   \n",
       "4            1      216.71      216.37      215.65        8.65       29.49   \n",
       "\n",
       "   ...  18_currentR  18_currentS  18_currentT  total_activePower  \\\n",
       "0  ...        18.70         9.71        25.52           35170.41   \n",
       "1  ...        11.18        29.47        13.38           40919.00   \n",
       "2  ...        23.18        26.97        24.94           40123.25   \n",
       "3  ...        28.25        19.21         6.53           40927.77   \n",
       "4  ...         9.19        19.51        16.25           38327.94   \n",
       "\n",
       "   total_operation            datetime  hour  day  weekday  is_weekend  \n",
       "0               13 2024-12-01 08:00:00     8    1        6           1  \n",
       "1               13 2024-12-01 08:00:05     8    1        6           1  \n",
       "2               13 2024-12-01 08:00:10     8    1        6           1  \n",
       "3               13 2024-12-01 08:00:15     8    1        6           1  \n",
       "4               13 2024-12-01 08:00:20     8    1        6           1  \n",
       "\n",
       "[5 rows x 126 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_important = df_important.sort_values('timestamp').reset_index(drop=True)\n",
    "df_important.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation analysis starting...\n",
      "Correlation analysis completed! Time: 62.93s\n",
      "Top 50 correlations with target:\n",
      "==================================================\n",
      " 2. 16_activePower                      | 0.2787\n",
      " 3. 3_activePower                       | 0.2783\n",
      " 4. 2_activePower                       | 0.2779\n",
      " 5. 11_activePower                      | 0.2776\n",
      " 6. 15_activePower                      | 0.2776\n",
      " 7. 14_activePower                      | 0.2773\n",
      " 8. 1_activePower                       | 0.2773\n",
      " 9. 13_activePower                      | 0.2772\n",
      "10. 18_activePower                      | 0.2771\n",
      "11. 4_activePower                       | 0.2769\n",
      "12. 12_activePower                      | 0.2768\n",
      "13. 17_activePower                      | 0.2765\n",
      "14. 5_activePower                       | 0.2762\n",
      "15. 2_currentT                          | 0.1613\n",
      "16. 16_currentS                         | 0.1610\n",
      "17. 3_currentT                          | 0.1609\n",
      "18. 16_currentT                         | 0.1609\n",
      "19. 16_currentR                         | 0.1608\n",
      "20. 14_currentR                         | 0.1607\n",
      "21. 15_currentR                         | 0.1607\n",
      "22. 3_currentS                          | 0.1607\n",
      "23. 11_currentR                         | 0.1605\n",
      "24. 1_currentS                          | 0.1605\n",
      "25. 18_currentS                         | 0.1604\n",
      "26. 12_currentR                         | 0.1604\n",
      "27. 12_currentT                         | 0.1604\n",
      "28. 18_currentT                         | 0.1604\n",
      "29. 13_currentS                         | 0.1603\n",
      "30. 15_currentS                         | 0.1603\n",
      "31. 11_currentS                         | 0.1602\n",
      "32. 4_currentR                          | 0.1602\n",
      "33. 2_currentR                          | 0.1601\n",
      "34. 14_currentS                         | 0.1600\n",
      "35. 3_currentR                          | 0.1600\n",
      "36. 1_currentR                          | 0.1600\n",
      "37. 5_currentS                          | 0.1599\n",
      "38. 2_currentS                          | 0.1599\n",
      "39. 17_currentT                         | 0.1598\n",
      "40. 13_currentT                         | 0.1598\n",
      "41. 4_currentS                          | 0.1598\n",
      "42. 11_currentT                         | 0.1597\n",
      "43. 17_currentR                         | 0.1596\n",
      "44. 13_currentR                         | 0.1595\n",
      "45. 1_currentT                          | 0.1594\n",
      "46. 15_currentT                         | 0.1594\n",
      "47. 4_currentT                          | 0.1594\n",
      "48. 5_currentR                          | 0.1593\n",
      "49. 14_currentT                         | 0.1592\n",
      "50. 17_currentS                         | 0.1592\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tqdm\n",
    "\n",
    "print(\"Correlation analysis starting...\")\n",
    "correlation_start = time.time()\n",
    "\n",
    "target_col = 'total_activePower'\n",
    "feature_cols = [col for col in df_important.columns if col not in ['timestamp', 'localtime', 'datetime', target_col]]\n",
    "\n",
    "correlations = df_important[feature_cols + [target_col]].corr()[target_col].abs().sort_values(ascending=False)\n",
    "\n",
    "correlation_end = time.time()\n",
    "print(f\"Correlation analysis completed! Time: {correlation_end - correlation_start:.2f}s\")\n",
    "\n",
    "print(\"Top 50 correlations with target:\")\n",
    "print(\"=\" * 50)\n",
    "for i, (col, corr) in enumerate(correlations.head(50).items()):\n",
    "    if col != target_col:\n",
    "        print(f\"{i+1:2d}. {col:<35} | {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected TOP 10 features:\n",
      "==================================================\n",
      " 1. 16_activePower                      | 0.2787\n",
      " 2. 3_activePower                       | 0.2783\n",
      " 3. 2_activePower                       | 0.2779\n",
      " 4. 11_activePower                      | 0.2776\n",
      " 5. 15_activePower                      | 0.2776\n",
      " 6. 14_activePower                      | 0.2773\n",
      " 7. 1_activePower                       | 0.2773\n",
      " 8. 13_activePower                      | 0.2772\n",
      " 9. 18_activePower                      | 0.2771\n",
      "10. 4_activePower                       | 0.2769\n"
     ]
    }
   ],
   "source": [
    "top_10_features = correlations.head(11).index.tolist()\n",
    "top_10_features.remove(target_col)\n",
    "top_10_features = top_10_features[:10]\n",
    "\n",
    "print(f\"Selected TOP 10 features:\")\n",
    "print(\"=\" * 50)\n",
    "for i, feature in enumerate(top_10_features, 1):\n",
    "    corr_value = correlations[feature]\n",
    "    print(f\"{i:2d}. {feature:<35} | {corr_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size reduction:\n",
      "   Original: (2592001, 126)\n",
      "   Reduced:  (2592001, 14)\n",
      "   Features: 126 -> 14\n",
      "   Reduction: 88.9%\n"
     ]
    }
   ],
   "source": [
    "selected_cols = ['timestamp', 'localtime', 'datetime'] + top_10_features + [target_col]\n",
    "df_reduced = df_important[selected_cols].copy()\n",
    "\n",
    "print(f\"Data size reduction:\")\n",
    "print(f\"   Original: {df_important.shape}\")\n",
    "print(f\"   Reduced:  {df_reduced.shape}\")\n",
    "print(f\"   Features: {len(df_important.columns)} -> {len(df_reduced.columns)}\")\n",
    "print(f\"   Reduction: {(1 - len(df_reduced.columns)/len(df_important.columns))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sliding windows...\n",
      "Total samples: 2,591,977\n",
      "Window size: 24, Features: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating windows:   7%|▋         | 188818/2591977 [00:00<00:03, 636048.76it/s]Creating windows: 100%|██████████| 2591977/2591977 [00:03<00:00, 653103.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windowing completed! Time: 4.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def split_sequences(data, window_size, target_col, feature_cols):\n",
    "   print(\"Creating sliding windows...\")\n",
    "   start_time = time.time()\n",
    "   \n",
    "   feature_data = data[feature_cols].values\n",
    "   target_data = data[target_col].values\n",
    "   \n",
    "   total_samples = len(data) - window_size\n",
    "   num_features = len(feature_cols)\n",
    "   \n",
    "   print(f\"Total samples: {total_samples:,}\")\n",
    "   print(f\"Window size: {window_size}, Features: {num_features}\")\n",
    "   \n",
    "   X = np.zeros((total_samples, window_size, num_features))\n",
    "   y = np.zeros(total_samples)\n",
    "   \n",
    "   for i in tqdm(range(total_samples), desc=\"Creating windows\"):\n",
    "       X[i] = feature_data[i:i+window_size]\n",
    "       y[i] = target_data[i+window_size]\n",
    "   \n",
    "   end_time = time.time()\n",
    "   print(f\"Windowing completed! Time: {end_time - start_time:.2f}s\")\n",
    "   \n",
    "   return X, y\n",
    "\n",
    "window_size = 24\n",
    "selected_feature_cols = [col for col in df_reduced.columns if col not in ['timestamp', 'localtime', 'datetime']]\n",
    "\n",
    "X, y = split_sequences(df_reduced, window_size, target_col, selected_feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windowing results:\n",
      "   X shape: (2591977, 24, 11)\n",
      "   y shape: (2591977,)\n",
      "   Memory usage: 5220.7 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Windowing results:\")\n",
    "print(f\"   X shape: {X.shape}\")\n",
    "print(f\"   y shape: {y.shape}\")\n",
    "print(f\"   Memory usage: {X.nbytes / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling completed!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_2d = X.reshape(-1, X.shape[-1])\n",
    "X_scaled_2d = scaler_X.fit_transform(X_2d)\n",
    "X_scaled = X_scaled_2d.reshape(X.shape)\n",
    "\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(\"Scaling completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split results:\n",
      "   Train: X=(2073581, 24, 11), y=(2073581,)\n",
      "   Test:  X=(518396, 24, 11), y=(518396,)\n",
      "   Train flat: (2073581, 264)\n",
      "   Test flat: (518396, 264)\n"
     ]
    }
   ],
   "source": [
    "split_idx = int(len(X_scaled) * 0.8)\n",
    "\n",
    "X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]\n",
    "y_train, y_test = y_scaled[:split_idx], y_scaled[split_idx:]\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "print(f\"Split results:\")\n",
    "print(f\"   Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"   Test:  X={X_test.shape}, y={y_test.shape}\")\n",
    "print(f\"   Train flat: {X_train_flat.shape}\")\n",
    "print(f\"   Test flat: {X_test_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost with GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [04:45:58] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [04:46:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - MAE: 0.0803, RMSE: 0.1005\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "print(\"Training XGBoost with GPU...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "   n_estimators=1000,\n",
    "   max_depth=6,\n",
    "   learning_rate=0.1,\n",
    "   tree_method='gpu_hist',\n",
    "   gpu_id=0,\n",
    "   random_state=42,\n",
    "   n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_flat, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test_flat)\n",
    "\n",
    "xgb_mae = mean_absolute_error(y_test, xgb_pred)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))\n",
    "\n",
    "print(f\"XGBoost - MAE: {xgb_mae:.4f}, RMSE: {xgb_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM with GPU...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.558254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 67320\n",
      "[LightGBM] [Info] Number of data points in the train set: 2073581, number of used features: 264\n",
      "[LightGBM] [Info] Start training from score 0.473476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM - MAE: 0.0802, RMSE: 0.1004\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "print(\"Training LightGBM with GPU...\")\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "        random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgb_model.fit(X_train_flat, y_train)\n",
    "lgb_pred = lgb_model.predict(X_test_flat)\n",
    "\n",
    "lgb_mae = mean_absolute_error(y_test, lgb_pred)\n",
    "lgb_rmse = np.sqrt(mean_squared_error(y_test, lgb_pred))\n",
    "\n",
    "print(f\"LightGBM - MAE: {lgb_mae:.4f}, RMSE: {lgb_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CatBoost with GPU...\n",
      "CatBoost - MAE: 0.0801, RMSE: 0.1003\n"
     ]
    }
   ],
   "source": [
    "import catboost as cb\n",
    "\n",
    "print(\"Training CatBoost with GPU...\")\n",
    "cat_model = cb.CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    depth=6,\n",
    "    learning_rate=0.1,\n",
    "    task_type='GPU',\n",
    "    random_seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "cat_model.fit(X_train_flat, y_train)\n",
    "cat_pred = cat_model.predict(X_test_flat)\n",
    "\n",
    "cat_mae = mean_absolute_error(y_test, cat_pred)\n",
    "cat_rmse = np.sqrt(mean_squared_error(y_test, cat_pred))\n",
    "\n",
    "print(f\"CatBoost - MAE: {cat_mae:.4f}, RMSE: {cat_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble - MAE: 0.0801, RMSE: 0.1003\n"
     ]
    }
   ],
   "source": [
    "ensemble_pred = (xgb_pred + lgb_pred + cat_pred) / 3\n",
    "\n",
    "ensemble_mae = mean_absolute_error(y_test, ensemble_pred)\n",
    "ensemble_rmse = np.sqrt(mean_squared_error(y_test, ensemble_pred))\n",
    "\n",
    "print(f\"Ensemble - MAE: {ensemble_mae:.4f}, RMSE: {ensemble_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN-GRU with GPU...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 04:58:04.075976: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2025-05-22 04:58:05.471681: I external/local_xla/xla/service/service.cc:168] XLA service 0x1385a0d40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-05-22 04:58:05.471720: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2025-05-22 04:58:05.479016: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1747889885.573560    5916 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3240/3240 [==============================] - 20s 5ms/step - loss: 0.0104 - mae: 0.0814 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 2/50\n",
      "3240/3240 [==============================] - 15s 5ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 3/50\n",
      "3240/3240 [==============================] - 15s 5ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 4/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 5/50\n",
      "3240/3240 [==============================] - 15s 5ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 6/50\n",
      "3240/3240 [==============================] - 16s 5ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 7/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 8/50\n",
      "3240/3240 [==============================] - 17s 5ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 9/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 10/50\n",
      "3240/3240 [==============================] - 16s 5ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 11/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0101 - val_mae: 0.0802\n",
      "Epoch 12/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 13/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0100 - val_mae: 0.0801\n",
      "Epoch 14/50\n",
      "3240/3240 [==============================] - 15s 4ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0100 - val_mae: 0.0801\n",
      "Epoch 15/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 16/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 17/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 18/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 19/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 20/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 21/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 22/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 23/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 24/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 25/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 26/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 27/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 28/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 29/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 30/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 31/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 32/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 33/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 34/50\n",
      "3240/3240 [==============================] - 15s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 35/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 36/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 37/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 38/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 39/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 40/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 41/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 42/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 43/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 44/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 45/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 46/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0100 - val_mae: 0.0801\n",
      "Epoch 47/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 48/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 49/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 50/50\n",
      "3240/3240 [==============================] - 14s 4ms/step - loss: 0.0101 - mae: 0.0801 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "16200/16200 [==============================] - 22s 1ms/step\n",
      "CNN-GRU - MAE: 0.0801, RMSE: 0.1002\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GRU, Dense, Dropout\n",
    "\n",
    "print(\"Training CNN-GRU with GPU...\")\n",
    "with tf.device('/GPU:0'):\n",
    "    cnn_gru_model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        GRU(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(25, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "cnn_gru_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history = cnn_gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cnn_gru_pred = cnn_gru_model.predict(X_test).flatten()\n",
    "\n",
    "cnn_gru_mae = mean_absolute_error(y_test, cnn_gru_pred)\n",
    "cnn_gru_rmse = np.sqrt(mean_squared_error(y_test, cnn_gru_pred))\n",
    "\n",
    "print(f\"CNN-GRU - MAE: {cnn_gru_mae:.4f}, RMSE: {cnn_gru_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Residual CNN-GRU with GPU...\n",
      "Epoch 1/50\n",
      "3240/3240 [==============================] - 43s 12ms/step - loss: 0.0117 - mae: 0.0838 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 2/50\n",
      "3240/3240 [==============================] - 35s 11ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 3/50\n",
      "3240/3240 [==============================] - 35s 11ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0101 - val_mae: 0.0801\n",
      "Epoch 4/50\n",
      "3240/3240 [==============================] - 37s 11ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 5/50\n",
      "3240/3240 [==============================] - 36s 11ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0100 - val_mae: 0.0800\n",
      "Epoch 6/50\n",
      "1113/3240 [=========>....................] - ETA: 21s - loss: 0.0101 - mae: 0.0803"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m     residual_model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39minput_layer, outputs\u001b[38;5;241m=\u001b[39moutput)\n\u001b[1;32m     38\u001b[0m     residual_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 40\u001b[0m history_residual \u001b[38;5;241m=\u001b[39m \u001b[43mresidual_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     46\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m residual_pred \u001b[38;5;241m=\u001b[39m residual_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     50\u001b[0m residual_mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_test, residual_pred)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Add, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "def residual_block(x, filters, kernel_size=3):\n",
    "    shortcut = x\n",
    "    \n",
    "    x = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters=filters, kernel_size=1, padding='same')(shortcut)\n",
    "    \n",
    "    x = Add()([x, shortcut])\n",
    "    x = tf.keras.activations.relu(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "print(\"Training Residual CNN-GRU with GPU...\")\n",
    "with tf.device('/GPU:0'):\n",
    "    input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 32)\n",
    "\n",
    "    x = GRU(50, return_sequences=False)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(25, activation='relu')(x)\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    residual_model = Model(inputs=input_layer, outputs=output)\n",
    "    residual_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history_residual = residual_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "residual_pred = residual_model.predict(X_test).flatten()\n",
    "\n",
    "residual_mae = mean_absolute_error(y_test, residual_pred)\n",
    "residual_rmse = np.sqrt(mean_squared_error(y_test, residual_pred))\n",
    "\n",
    "print(f\"Residual CNN-GRU - MAE: {residual_mae:.4f}, RMSE: {residual_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
    "\n",
    "print(\"Training Attention CNN-Residual-GRU with GPU...\")\n",
    "with tf.device('/GPU:0'):\n",
    "    input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 32)\n",
    "\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=4, \n",
    "        key_dim=32\n",
    "    )(x, x)\n",
    "\n",
    "    x = Add()([x, attention_output])\n",
    "    x = LayerNormalization()(x)\n",
    "\n",
    "    x = GRU(50, return_sequences=False)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(25, activation='relu')(x)\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    attention_model = Model(inputs=input_layer, outputs=output)\n",
    "    attention_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history_attention = attention_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "attention_pred = attention_model.predict(X_test).flatten()\n",
    "\n",
    "attention_mae = mean_absolute_error(y_test, attention_pred)\n",
    "attention_rmse = np.sqrt(mean_squared_error(y_test, attention_pred))\n",
    "\n",
    "print(f\"Attention CNN-Residual-GRU - MAE: {attention_mae:.4f}, RMSE: {attention_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL RESULTS COMPARISON\n",
      "============================================================\n",
      "XGBoost                  - MAE: 0.0803, RMSE: 0.1005\n",
      "LightGBM                 - MAE: 0.0802, RMSE: 0.1004\n",
      "CatBoost                 - MAE: 0.0801, RMSE: 0.1003\n",
      "Ensemble (XGB+LGB+CAT)   - MAE: 0.0801, RMSE: 0.1003\n",
      "CNN-GRU                  - MAE: 0.0801, RMSE: 0.1002\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FINAL RESULTS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"XGBoost                  - MAE: {xgb_mae:.4f}, RMSE: {xgb_rmse:.4f}\")\n",
    "print(f\"LightGBM                 - MAE: {lgb_mae:.4f}, RMSE: {lgb_rmse:.4f}\")\n",
    "print(f\"CatBoost                 - MAE: {cat_mae:.4f}, RMSE: {cat_rmse:.4f}\")\n",
    "print(f\"Ensemble (XGB+LGB+CAT)   - MAE: {ensemble_mae:.4f}, RMSE: {ensemble_rmse:.4f}\")\n",
    "print(f\"CNN-GRU                  - MAE: {cnn_gru_mae:.4f}, RMSE: {cnn_gru_rmse:.4f}\")\n",
    "# print(f\"Residual-CNN-GRU         - MAE: {residual_mae:.4f}, RMSE: {residual_rmse:.4f}\")\n",
    "# print(f\"Attention-CNN-Res-GRU    - MAE: {attention_mae:.4f}, RMSE: {attention_rmse:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
