1) 인공지능에서 지능에 해당되는 기능
인공지능은 인간의 지적 능력을 컴퓨터나 기계 시스템에 구현하여 학습, 추론, 판단, 의사 결정 등 인간의 지능과 유사한 기능을 수행하는 기술이라고 말한다. 인간의 특정 능력을 분류, 회귀, 인식 등을 기계가 하게끔 하는 것이다.

2) 인공지능의 종류 3가지 (지도학습, 반지도학습, 강화학습)
인공지능은 target (원래 데이터의 정답)의 유무에 따라서 지도학습과 비지도학습으로 나뉜다. 지도학습은 data와 target을 모델에 주어 data와 target 사이의 관계를 학습하게 되어 추후 predict 시 test data를 모델에 넣었을 때 예상 정답을 도출해내게 하는 방법이다. 비지도학습은 target이 존재하지 않은 채 오직 data끼리의 유사성에 따라서 clustering 등을 수행하는 방법이다. 반지도학습이란 지도학습과 비지도학습의 융합된 학습 방법으로써, 레이블이 있는 데이터와 없는 데이터를 모두 활용하여 머신러닝 모델을 학습시키는 방법이다. 비지도학습으로 특정 데이터의 군집을 찾고, 레이블이 있는 데이터를 이용하여 지도학습을 한다. 이를 통하여 모델의 성능을 높일 수 있다. 데이터를 섞거나, 모델을 섞을 수도 있다. (일부분은 label을 알려주어서 정확도를 높일 수 있다.) 강화학습이란 환경과 에이전트를 만들어서, 에이전트에게 상벌을 내려 목표 상황에 도달할 수 있도록 하게 하는 학습 방법이다. (상을 받는 방향으로)

3) 전통적인 프로그래밍 vs 인공지능 프로그래밍
전통적인 프로그래밍은 특정 상황에서 어떻게 동작할 지 명령하는 일종의 rule을 직접 전부 작성하였어야 했다. 그니깐, 데이터와 rule을 대입하여 출력을 만든 것이 이전의 방식이라면, 인공지능 프로그래밍에서는 데이터와 그에 매핑되는 target을 모델에 넣어서 규칙을 구한다. 

4) 딥러닝 vs 머신러닝
머신러닝은 사람이 직접 특징을 추출해서 대입을 하였지만, 딥러닝은 특징 추출을 스스로 한다는 점에서 다르다. 그니깐, feature extraction한 데이터를 대입하지 않고 원시적인 데이터를 모델에 넣어도 특징 추출을 스스로 한다는 것이다.

5) curse of dimensionality
데이터프레임에서 column에 해당되는 것을 feature라고 한다. 예를 들어서 Abalone 데이터에서 Age, height, length 등이 그 데이터의 feature이다. 결국에 모델은 데이터를 가장 잘 대표하고 표현할 수 있는 가중치와 bias를 찾는 것이 목표인데, feature가 너무 많아지면 gradient의 양 또한 많아져 (차원이 늘어나) 최적의 가중치를 탐색하는 것이 어려울 수 있다. 이 상황을 차원의 저주라고 하며, 본 상황을 해결할 수 있는 방법은 regularization이나 feature selection이 있을 수 있겠다. 또한 차원을 축소하여 (PCA) 데이터 복잡성을 줄이는 것도 이에 대한 해결 방법이다. 결국 차원의 저주는 모델의 overfitting을 야기할 수 있으며 모델이 데이터를 이해하기 어려워진다.

6) dimensionality reduction
위와 같은 차원의 저주와 같은 문제를 해결하기 위한 일종의 방법이 차원 축소이다. dimensionality reduction은 말 그대로 데이터의 차원, feature을 줄이는 방법이며, regularization, feature selection, PCA가 있다. regularization은 데이터의 일부 feature을 0에 가깝게 하거나 (L2), 아예 0으로 만드는 (L1) 것이다. feature selection은 target과 상관계수 (corr)가 높은 feature만 선택하여 데이터를 이루는 것이다. PCA는 차원 축소 알고리즘 중 하나로 주성분을 이용하여 사용자가 하이퍼파라미터로 대입한 차원으로 데이터를 축소하게 된다.

7) ridge, lasso, regularization, scaling
regularization은 결국엔 overfitting을 방지하기 위함이다. overfitting은 모델이 train 데이터에서의 성능은 뛰어나나, 새로운 데이터인 test 데이터에서의 성능은 좋지 않은 상황을 말한다. 즉 모델의 일반화 성능이 뛰어나지 않음을 말한다. ridge는 제곱항 규제, L2 regularization라고 불리우며, 이는 feature을 0에 가깝게 한다. lasso는 절댓값 규제, L1 regularization라고 불리우며 이는 feature을 아예 0으로 만든다. 둘 다 규제 알고리즘으로써 feature에 대한 가중치의 값을 0에 수렴 혹은 0으로 만들어서 복잡도를 줄인다는 것은 공통점이 된다. 다만 계산 방법과 policy가 다른 것이다. 

scaling은 데이터의 각 feature에 있는 값들의 분포를 서로 일정하게 맞추어서 모델이 데이터를 잘 파악할 수 있도록 하는 기법이다. data preprocessing 과정에서의 scaling 기법은 크게 StandardScaler, MinMaxScaler이다. StandardScaler은 흔히 우리가 말하는 basic standardization (Z-score normalization)을 하는 API이다. MinMaxScaler은 최솟값 및 최댓값에 대한 scaliing을 말한다. scaling을 하는 이유를 한 예시로 설명해보자면, 만약에 거리를 metrics로 두는 사례기반 모델이 있다고 가정하자. scaling을 하지 않으면 당연이 component 값이 큰 feature에 대하여 모델에 더욱 많은 영향을 미칠 수 있다. feature을 공평하게 대입하여 학습을 진행할 수 있도록 scaling하여야 한다.

8) overfitting, underfitting
overfitting은 언급된 바와 같이, train 데이터를 통한 학습에서는 train loss은 낮게 형성되어 제대로 잘 학습되나, 학습에서 보지 못한 새로운 데이터인 test 데이터에서의 predict에서는 성능이 좋지 않은 상황을 말한다. 즉 모델의 일반화 성능이 좋지 않은 상황을 말한다. overfitting이 된 모델은 train 데이터에서만 딱 맞춰진 모델이기 때문에 noise까지 학습 및 outlier까지 학습한 것이기 때문에 잘못된 학습인 것이다. overfitting의 원인은 데이터의 차원이 크거나 (curse of dimensionality), 모델의 복잡도가 너무 높을 때 등에서 발생한다. 이에 대한 원시적인 해결 방안은 데이터의 노이즈를 줄이거나 모델의 복잡도를 낮추면 된다. 모델 내에서 해결할 수 있는 방안은 dropout, batch normalization, regularization 등이 있다. 

underfitting은 train 데이터 또한 제대로 학습하지 못하여 score가 낮은 상황을 말한다. 해결책은 데이터 증대, 모델의 복잡도 증가, 규제양 감소가 있다.

9) feature engineering과 feature selection의 차이점
feature engnieering은 특성공학으로, feature의 통계값, feature들 사이의 상관계수 등 feature들끼리 조합하여 예측에 도움이 되는 새로운 feature을 만드는 방법이다. feature selection은 target과의 상관관계가 높은 몇개의 feature을 뽑아서 data를 구성하는 것으로, curse of dimensionality를 피하게 할 수 있는 방법 중 하나이다. feature selection을 하면 차원 축소를 통해 overfitting을 막을 수 있다.

10) 노이즈, 이상치, 결측치
노이즈란 기존 데이터 분포와 동떨어지면서도 데이터의 originality를 훼손하는 데이터포인트를 말한다. 이상치는 데이터 분포와 멀리 떨어져 모델의 overfitting을 야기할 수 있는 데이터포인트를 말한다. 이상치는 boxplot을 그려서 사분위수와 함께 판단할 수 있다. 결측치는 데이터에서 특정 component가 비어있는 nan, null값이다. 이는 fillna, dropna로 대처할 수 있다.

11) EDA
EDA는 "탐색적 데이터 분석(Exploratory Data Analysis)"라고 불리우며, 데이터 분석의 초기 단계에서 데이터를 탐색하고 이해하는 과정을 의미한다. EDA는 통계적 가설을 검증하는 것을 목표로 하지 않고, 데이터를 시각화하고 통계적으로 요약하여 데이터의 특징과 패턴을 실험자가 파악하는 과정이다.

12) 회귀에서 절편과 기울기가 의미하는 바는? 딥러닝과 어떻게 연관이 되는가?
regression에서 기울기는 feature input Xi의 계수이고, 절편은 상수항의 역할을 한다. 일반적인 ML에서는 normal equation으로 이들을 수학적으로 구할 수 있지만, DL에서는 forward propagation -> backward propagation -> update gradient 과정을 통하여 절편과 기울기를 업데이트해주어야 한다. 이는 loss를 최소화하는 최적화알고리즘인 경사하강법을 통하여 매 epoch마다 update된다. 

13) 교차검증, k-fold 교차검증
교차검증은 모델의 일반화 성능을 평가하기 위해 데이터를 여러 번 나누어 학습하고, 검증하는 방법이다. 이는 데이터를 학습 세트와 검증 세트로 나누어, 모델의 overftting을 방지한다. K-fold 교차검증은 데이터를 K개의 폴드로 나누어, 각 폴드마다 한 번씩 검증 세트로 사용하고, 나머지는 학습 세트로 사용하는 방법으로, 이를 통해 K번의 학습과 검증을 수행하여, 모델의 평균 성능을 평가한다. 교차검증은 단순히 데이터를 나누어 학습하고 검증하는 반면, K-fold 교차검증은 데이터를 K개의 폴드로 나누어 여러 번 학습과 검증을 반복하는 것이다. cross validation을 함으로써 모델이 특정 세트에 대하여 편향적으로 학습되는 것을 막는 것이 이의 목적이다.

14) 하이퍼파라미터 튜닝
layer에서 뉴런의 수, activation의 종류, optimizer, CNN에서의 커널의 수 등 개발자가 직접 관여하여 조정할 수 있는 파라미터를 하이퍼파라미터라고 한다. 이 하이퍼파라미터의 조합을 어떻게 하는지에 따라서 모델의 성능은 달라질 수 있다. 튜닝을 하는 방법은 대표적으로 그리드 서치와 랜덤 서치가 있다. 

그리드 서치는 하이퍼파라미터 종류 별로 선택할 수 있는 집합을 각각 생성하고, 그것에 대한 모든 부분집합을 실험하여 가장 성능이 좋은 조합에서의 모델을 best model로 선정한다. 그리드 서치는 모든 조합을 실험하기에 (brute-force) 가장 많은 경우의수로 풍부한 성능 조사를 할 수 있다는 장점이 있으나, 시간이 오래 걸린다. 

랜덤 서치는 하이퍼파라미터의 범위를 정하여서 그 범위 내의 특정 값을 랜덤으로 뽑아 생성한 n개의 하이퍼파라미터 조합으로 실험을 진행하는 것이다. 이는 그리드 서치의 시간적 대안책으로 가장 많이 사용하는 튜닝 기법이다.

15) 결정트리에서 불순도와 지니계수가 무엇인가
Decision tree는 0과 1, True와 False, 즉 binary tree를 계속 그려나가 분류 및 회귀를 진행하는 모델이다. 질문 (test)를 만들기 위한 학습을 진행하여, 각 class별로 효과적으로 분류하기 위한 모델이다. 이때 모델은 불순도가 최대한 0에 가깝게 구성해나가는 것이 핵심인데, 여기서 불순도는 얼마나 class가 서로 섞였는지를 나타내는 수치이다. 예를 들어서 검정 공 3개, 하얀 공 3개라면 불순도는 0.5이다. gini impurity는 현재 node에 들어온 각 component들의 class가 얼마나 섞여 있는지를 나타낸다. 0.5에 가까우면 불순도가 높은 것이며, 0이면 불순도가 낮은 것이다. 

16) 앙상블
ensemble은 다양한 모델을 동시에 돌려서 그 중 성능 좋은 걸 선택하는 모델을 사용하는 기법이다. 앙상블의 대표적인 예시는 random forest이다. 여러 개의 결정 트리를 부트스트래핑하고, 랜덤한 feature 선택으로 그 결과를 투표 또는 평균으로 결합해 overfitting은 줄이고 성능을 높인 모델이다.

17) 부트스트래핑
원본 데이터에서 중복을 허용하면서 랜덤하게 샘플을 뽑는 방법으로써, 복원 추출을 하는 것이다. 데이터가 100개면 그 중 100개를 중복 허용해서 다시 뽑는 방식으로, 어떤 데이터는 여러 번 뽑히고, 어떤 건 빠질 수도 있다. 부트스트래핑은 random forest에서 데이터를 뽑는 방법인데, 만약 뽑히지 않은 데이터는 OOB가 되어서 검증에서 사용하게 된다.

18) 베깅
부트스트래핑으로 여러 모델을 만들고, 그 결과를 평균(회귀) 또는 투표(분류)로 합치는 방법을 말한다. 예시로 여러 개의 DT를 만들고 난 후 예측 결과를 종합하여서 더욱 안정된 예측을 만드는 방식이 베깅이다.

19) 주성분 분석
주성분 분석은 curse of dimensionality를 해결할 수 있는 dimension reduction 기법 중 하나로써, 정보의 압축 및 차원을 축소하게 된다. 주성분을 찾아서 차원을 감소시키는 방법, 데이터의 분산이 최대가 되는 축을 찾아서 축소하게 된다. n차원의 공간을 n-p차원으로 줄이기 위해서 p+1개의 과정이 필요하다. (분산이 최대 -> hyperparameter나 모델 파라미터가 조금이라도 변해도 새로운 모델이 만들어질 수 있음)

------------------

20) 교차검증을 하는 이유
cross validation의 목적은 train dataset에 편향되어 학습하고자 하는 것을 막기 위하여 val set을 두어서 모델이 특정 set에 대해 편향된 학습을 하는 것을 막기 위함이다.

21) confusion matrix, acc, f1, precision, recall, 민감도, 특이도
모델이 train을 다 한 후, test 데이터 (학습에서 보지 못했던 새로운 데이터) 를 통해 모델의 일반화 성능을 확인할 때 모델의 성능 지표가 있다. classification 문제에서는 target이 label로 되어있기 때문에 confusion matrix를 그려서 어느 label에서 모델이 잘 예측하였고, 잘 예측하지 못하였는지를 판단할 수 있다. confusion matrix는 아래와 같이 생겼다.

| TP | FN | 
| FP | TN | 

위 confusion matrix의 각 숫자를 이용하여 accuracy, f1 score, precision, recall, speciality을 구할 수 있다. accuracy는 (TP+TN)/(TP+TN+FP+FN)으로 전체 데이터 중 예측이 얼마나 잘 맞았는가를 확인할 수 있는 metrics이다. precision은 (TP)/(TP+FP), recall이 (TP)/(TP+FN)이고, f1 score은 2 * (precision * recall) / precision+recall이다. 

| 1000 | 10 |
| 10     | 10 |

f1 score이 중요한 이유는 위 예시를 통해서 알 수 있다. 위 confusion matrix를 이용하여 accuracy를 구한다면 1010/1030으로 매우 높은 값인 것을 알 수 있다. 하지만 이 데이터에서 중요한 거는 레이블 0이 아니라 레이블 1이다. 이렇게 레이블 편향된 데이터가 있을 때 실질적으로 중요한 건 위에서 label 1을 모델이 얼마나 잘 예측하였는가이다. 따라서 F1 score을 구하면 편향된 데이터의 성능을 보고 모델의 학습 성능을 파악할 수 있다.

22) loss function 3가지
BCE, MSE, CE
BCE는 binary crossentropy의 줄임말로써, binary classification 시에 사용하는 loss function이다. 주로 tensorflow에서는 model.compile 안에서 사용하는 하이퍼파라미터로써, 기존의 이진 답을 가지고 있는 데이터가 [0, 1], [1, 0]으로 one-hot encoding되어 있어야 한다.

MSE는 mean squared error의 줄임말로써, 실제 정답값과 예측값의 오차를 제곱하여 합한 값을 의미한다. MSE에서 제곱을 하는 이유는 만약 loss, 즉 오차가 심하게 발생할 시에 이에 대하여 더욱 큰 패널티를 주기 위함이다. MSE는 보통 regression 문제에서 많이 사용하는 loss function이다.

CE는 categorical cross entrpy의 줄임말로써, 다중 clasisfication 문제에서 사용하는 loss function이다. 데이터의 target label들은 모두 one-hot encoding되어 있어야만 본 함수를 사용할 수 있고, 만약 one-hot encoding이 되어있지 않다면 sparse_categorical_crossentropy를 사용하여야만 한다.

23) back propagation
back propagation은 한국어로 역전파 알고리즘이라고 불리우며, 이는 순전파의 반대이다. forward propagation은 이전 단계 (혹은 초기 단계)에서의 모델에 데이터를 대입하는 과정이고, 이 과정에서 예측값이 출력되었으면 예측값과 실제값의 오차, 즉 loss를 최소화하기 위하여 weight와 bias를 업데이트하여야 한다. back propagation은 편미분을 사용하여 최적의 weight와 bias를 업데이트하는 과정이라고 볼 수 있다. back propagation을 함으로써 경사하강법 수식인 Wt = Wt-1 + r * f'을 이용하여 gradient를 업데이트하게 된다.

24) 시그모이드, relu 함수, 계단 함수
시그모이드, relu, 계단 함수 모두 activaton function인데, activation function은 중간에 위치하여 흐름에 대한 비선형성을 제공해주는 함수이다. 만약 함수가 선형적으로 흘러간다고 가정해보자. 그러면 f(x)가 계속적으로 쌓인다고 볼 수 있겠다. 만약 한 layer를 f(x)라고 한다면, layer 3개를 선형적으로 놓았을 시 3f(x)가 된다. 계수를 무시한다고 가정하였을 때, 이는 출력값과 입력값이 동일해지므로 layer를 놓는 과정이 의미가 없어진다. 따라서 각 layer에 activation, 즉 비선형성을 주어서 조금 더 복잡한 특징을 추출하기 위함으로 사용한다. (feature extraction을 더욱 잘 하기 위함이다.)

25) train test val로 나누는 이유
train dataset은 모델의 학습을 위한 데이터셋이고, 모델이 특정 set에만 편향되어 학습하는 것을 방지하기 위하여 cross-validation을 위한 val set을 두어서 모델 epoch가 끝날 때 valiidate하여 모델의 성능을 파악하게 된다. (만약에 K개의 선형부분집합으로 val set과 train set을 번갈아서 선택하여 검증하고 싶으면 K-fold cross validation을 사용하면 된다) test는 모델의 일반화 성능을 확인하기 위해 두는 dataset이다.

26) 선형회귀와 다중회귀의 차이점
선형회귀는 모델의 입력이 하나일 때, 그니깐 Xi에 해당되는 i가 하나일 때 사용하는 regression 알고리즘이다. X와 Y를 가장 잘 표현할 수 있는 선 (Hypothesis = Wx+b) 하나를 찾는 것이다. 다중회귀는 모델의 입력이 여러 개일때, 그니깐 Xi에 해당되는 i가 하나일 때 사용하는 regression 알고리즘이다. X가 다차원이기 때문에 이제 Xi 집합과 Y를 가장 잘 표현하는 것은 plane 모양이 될 것이다.

27) 확률적 경사하강법, 배치경사하강법, 미니배치경사하강법
배치경사하강법은 모든 데이터를 이용하여 bias와 weight를 구하는 알고리즘으로써, 장점으로는 특이성에 민감하지는 않지만, 시간이 오래 걸리고, function의 개형이 convex하지 않으면 local minima에 수렴하는 문제가 발생할 수 있다. 

SGD (확률적경사하강법)은 매 스텝에서 한 샘플을 랜덤으로 선택하고 그에 대한 weight와 bias를 구하는 것이다. 장점은 시간이 적게 걸리지만, 단점으로는 랜덤성이 있기 때문에 global minima로 갈 것이라는 보장은 할 수 없다.

minibatch 경사하강법은 데이터셋을 일정한 크기의 미니배치 단위로 나누고, 한 배치를 택하여 학습을 진행한다. 일종의 online 학습이 가능하며, 장점으로는 적은 데이터를 사용하기 때문에 한정적 자원에서 유리하며 GPU를 이용하여 병렬연산이 가능해진다.