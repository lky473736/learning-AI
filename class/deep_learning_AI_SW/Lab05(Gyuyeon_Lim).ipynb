{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning (Andrew Jaeyong Choi Prof.)\n",
        "### Lab. LSTM\n",
        "\n",
        "- Gyuyeon Lim (202334734, Department of Computer Engineering at Gachon Univ.)\n",
        "- lky473736@gmail.com\n",
        "\n",
        "------\n",
        "\n",
        "<br>\n",
        "\n",
        "### Previous Provided Code\n",
        "\n",
        "The original RNN model implements a simple recurrent neural network for next-character prediction in English words. The architecture consists of a single RNN layer with 26 hidden units, matching the number of alphabet characters. The model takes one-hot encoded characters as input and predicts the next character in the sequence. During training, it processes 50 five-letter English words for 900 epochs using Adam optimizer with a learning rate of 0.001 and a step scheduler that reduces the learning rate by a factor of 0.1 every 300 epochs. The RNN maintains a hidden state that captures sequential dependencies, but due to the vanishing gradient problem inherent in vanilla RNNs, it struggles with longer-term dependencies. The model uses CrossEntropyLoss for training and evaluates performance by comparing predicted characters with ground truth at each position, calculating both final character accuracy and overall character-by-character accuracy across all test words."
      ],
      "metadata": {
        "id": "pmfrqyuxNxcS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8tUCj-RMv2v",
        "outputId": "568f4dbd-cc7a-44c7-b5d3-5a947c978ef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch0\n",
            "Loss: 3.2184\n",
            "epoch10\n",
            "Loss: 2.7669\n",
            "epoch20\n",
            "Loss: 2.6014\n",
            "epoch30\n",
            "Loss: 2.4702\n",
            "epoch40\n",
            "Loss: 2.3670\n",
            "epoch50\n",
            "Loss: 2.2834\n",
            "epoch60\n",
            "Loss: 2.2147\n",
            "epoch70\n",
            "Loss: 2.1603\n",
            "epoch80\n",
            "Loss: 2.1166\n",
            "epoch90\n",
            "Loss: 2.0801\n",
            "epoch100\n",
            "Loss: 2.0490\n",
            "epoch110\n",
            "Loss: 2.0216\n",
            "epoch120\n",
            "Loss: 1.9966\n",
            "epoch130\n",
            "Loss: 1.9738\n",
            "epoch140\n",
            "Loss: 1.9533\n",
            "epoch150\n",
            "Loss: 1.9349\n",
            "epoch160\n",
            "Loss: 1.9184\n",
            "epoch170\n",
            "Loss: 1.9034\n",
            "epoch180\n",
            "Loss: 1.8899\n",
            "epoch190\n",
            "Loss: 1.8774\n",
            "epoch200\n",
            "Loss: 1.8660\n",
            "epoch210\n",
            "Loss: 1.8559\n",
            "epoch220\n",
            "Loss: 1.8462\n",
            "epoch230\n",
            "Loss: 1.8377\n",
            "epoch240\n",
            "Loss: 1.8301\n",
            "epoch250\n",
            "Loss: 1.8227\n",
            "epoch260\n",
            "Loss: 1.8160\n",
            "epoch270\n",
            "Loss: 1.8096\n",
            "epoch280\n",
            "Loss: 1.8039\n",
            "epoch290\n",
            "Loss: 1.7982\n",
            "epoch300\n",
            "Loss: 1.7926\n",
            "epoch310\n",
            "Loss: 1.7901\n",
            "epoch320\n",
            "Loss: 1.7895\n",
            "epoch330\n",
            "Loss: 1.7889\n",
            "epoch340\n",
            "Loss: 1.7883\n",
            "epoch350\n",
            "Loss: 1.7877\n",
            "epoch360\n",
            "Loss: 1.7871\n",
            "epoch370\n",
            "Loss: 1.7864\n",
            "epoch380\n",
            "Loss: 1.7858\n",
            "epoch390\n",
            "Loss: 1.7852\n",
            "epoch400\n",
            "Loss: 1.7845\n",
            "epoch410\n",
            "Loss: 1.7839\n",
            "epoch420\n",
            "Loss: 1.7833\n",
            "epoch430\n",
            "Loss: 1.7827\n",
            "epoch440\n",
            "Loss: 1.7820\n",
            "epoch450\n",
            "Loss: 1.7814\n",
            "epoch460\n",
            "Loss: 1.7808\n",
            "epoch470\n",
            "Loss: 1.7802\n",
            "epoch480\n",
            "Loss: 1.7796\n",
            "epoch490\n",
            "Loss: 1.7790\n",
            "epoch500\n",
            "Loss: 1.7784\n",
            "epoch510\n",
            "Loss: 1.7778\n",
            "epoch520\n",
            "Loss: 1.7772\n",
            "epoch530\n",
            "Loss: 1.7765\n",
            "epoch540\n",
            "Loss: 1.7759\n",
            "epoch550\n",
            "Loss: 1.7753\n",
            "epoch560\n",
            "Loss: 1.7747\n",
            "epoch570\n",
            "Loss: 1.7741\n",
            "epoch580\n",
            "Loss: 1.7735\n",
            "epoch590\n",
            "Loss: 1.7730\n",
            "epoch600\n",
            "Loss: 1.7723\n",
            "epoch610\n",
            "Loss: 1.7722\n",
            "epoch620\n",
            "Loss: 1.7722\n",
            "epoch630\n",
            "Loss: 1.7721\n",
            "epoch640\n",
            "Loss: 1.7721\n",
            "epoch650\n",
            "Loss: 1.7720\n",
            "epoch660\n",
            "Loss: 1.7720\n",
            "epoch670\n",
            "Loss: 1.7719\n",
            "epoch680\n",
            "Loss: 1.7718\n",
            "epoch690\n",
            "Loss: 1.7718\n",
            "epoch700\n",
            "Loss: 1.7717\n",
            "epoch710\n",
            "Loss: 1.7717\n",
            "epoch720\n",
            "Loss: 1.7716\n",
            "epoch730\n",
            "Loss: 1.7716\n",
            "epoch740\n",
            "Loss: 1.7715\n",
            "epoch750\n",
            "Loss: 1.7714\n",
            "epoch760\n",
            "Loss: 1.7714\n",
            "epoch770\n",
            "Loss: 1.7713\n",
            "epoch780\n",
            "Loss: 1.7713\n",
            "epoch790\n",
            "Loss: 1.7712\n",
            "epoch800\n",
            "Loss: 1.7712\n",
            "epoch810\n",
            "Loss: 1.7711\n",
            "epoch820\n",
            "Loss: 1.7711\n",
            "epoch830\n",
            "Loss: 1.7710\n",
            "epoch840\n",
            "Loss: 1.7709\n",
            "epoch850\n",
            "Loss: 1.7709\n",
            "epoch860\n",
            "Loss: 1.7708\n",
            "epoch870\n",
            "Loss: 1.7708\n",
            "epoch880\n",
            "Loss: 1.7707\n",
            "epoch890\n",
            "Loss: 1.7707\n",
            "1 GT:basic OUT:besic ACC:0.750\n",
            "2 GT:beach OUT:besch ACC:0.750\n",
            "3 GT:below OUT:besow ACC:0.750\n",
            "4 GT:black OUT:besck ACC:0.500\n",
            "5 GT:brown OUT:beswn ACC:0.500\n",
            "6 GT:carry OUT:crnry ACC:0.500\n",
            "7 GT:cream OUT:crrnm ACC:0.500\n",
            "8 GT:drink OUT:drink ACC:1.000\n",
            "9 GT:error OUT:eaeor ACC:0.500\n",
            "10 GT:event OUT:eaent ACC:0.750\n",
            "11 GT:exist OUT:eaict ACC:0.500\n",
            "12 GT:first OUT:first ACC:1.000\n",
            "13 GT:funny OUT:finny ACC:0.750\n",
            "14 GT:guess OUT:guesc ACC:0.750\n",
            "15 GT:human OUT:honan ACC:0.500\n",
            "16 GT:image OUT:image ACC:1.000\n",
            "17 GT:large OUT:large ACC:1.000\n",
            "18 GT:magic OUT:magic ACC:1.000\n",
            "19 GT:mouse OUT:mauct ACC:0.250\n",
            "20 GT:night OUT:noiht ACC:0.500\n",
            "21 GT:noise OUT:noist ACC:0.750\n",
            "22 GT:ocean OUT:oreen ACC:0.500\n",
            "23 GT:often OUT:orten ACC:0.750\n",
            "24 GT:order OUT:order ACC:1.000\n",
            "25 GT:peace OUT:prace ACC:0.750\n",
            "26 GT:phone OUT:prine ACC:0.500\n",
            "27 GT:print OUT:print ACC:1.000\n",
            "28 GT:quiet OUT:quict ACC:0.750\n",
            "29 GT:reach OUT:roach ACC:0.750\n",
            "30 GT:rough OUT:rounh ACC:0.750\n",
            "31 GT:round OUT:round ACC:1.000\n",
            "32 GT:scene OUT:scene ACC:1.000\n",
            "33 GT:score OUT:scere ACC:0.750\n",
            "34 GT:sense OUT:scnse ACC:0.750\n",
            "35 GT:skill OUT:scile ACC:0.500\n",
            "36 GT:sleep OUT:sceee ACC:0.500\n",
            "37 GT:small OUT:scole ACC:0.250\n",
            "38 GT:storm OUT:score ACC:0.500\n",
            "39 GT:table OUT:toble ACC:0.750\n",
            "40 GT:think OUT:tounk ACC:0.500\n",
            "41 GT:touch OUT:tornh ACC:0.500\n",
            "42 GT:twice OUT:toice ACC:0.750\n",
            "43 GT:until OUT:uptil ACC:0.750\n",
            "44 GT:upset OUT:uphee ACC:0.500\n",
            "45 GT:voice OUT:voice ACC:1.000\n",
            "46 GT:waste OUT:watct ACC:0.250\n",
            "47 GT:watch OUT:watch ACC:1.000\n",
            "48 GT:white OUT:waite ACC:0.750\n",
            "49 GT:woman OUT:wamat ACC:0.500\n",
            "50 GT:young OUT:young ACC:1.000\n",
            "final text accuracy 40/50 (0.8000)\n",
            "whole text accuracy 139/200 (0.6950)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "chars = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "char_list = [i for i in chars]\n",
        "n_letters = len(char_list)\n",
        "\n",
        "n_layers = 1\n",
        "\n",
        "five_words = ['basic','beach','below','black','brown','carry','cream','drink','error','event','exist','first',\n",
        "              'funny','guess','human','image','large','magic','mouse','night','noise','ocean','often','order',\n",
        "              'peace','phone','print','quiet','reach','rough','round','scene','score','sense','skill','sleep',\n",
        "              'small','storm','table','think','touch','twice','until','upset','voice','waste','watch','white','woman','young']\n",
        "n_five_words = len(five_words)\n",
        "\n",
        "sequence_length = 4\n",
        "\n",
        "def word_to_onehot(string):\n",
        "    one_hot = np.array([]).reshape(0,n_letters)\n",
        "    for i in string:\n",
        "      idx = char_list.index(i)\n",
        "      zero = np.zeros(shape=n_letters, dtype=int)\n",
        "      zero[idx] = 1\n",
        "      one_hot = np.vstack([one_hot, zero])\n",
        "    return one_hot\n",
        "\n",
        "def onehot_to_word(onehot_1):\n",
        "    onehot = torch.Tensor.numpy(onehot_1)\n",
        "    return char_list[onehot.argmax()]\n",
        "\n",
        "# Use RNN Packages\n",
        "class myRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layer):\n",
        "    super(myRNN,  self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layer = num_layer\n",
        "\n",
        "    self.rnn = nn.RNN(input_size = input_size,hidden_size=hidden_size, num_layers=num_layer)\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    out, hidden = self.rnn(x, hidden)\n",
        "    return out, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.num_layer, 1, self.hidden_size)\n",
        "\n",
        "# Lists to store loss and accuracy\n",
        "rnn_losses = []\n",
        "rnn_word_accuracies = []\n",
        "\n",
        "def main():\n",
        "  n_hidden = 26\n",
        "  lr = 0.001\n",
        "  epochs = 900\n",
        "\n",
        "  model = myRNN(n_letters, n_hidden, n_layers)\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 300, gamma=0.1)\n",
        "\n",
        "  for i in range(epochs):\n",
        "    total_loss = 0\n",
        "    for j in range(n_five_words):\n",
        "      hidden = model.init_hidden()\n",
        "      string = five_words[j]\n",
        "      one_hot = torch.from_numpy(word_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "      model.zero_grad()\n",
        "      hidden = model.init_hidden()\n",
        "      input = one_hot[0:-1]\n",
        "      input = torch.unsqueeze(input, 1)\n",
        "      target = np.argmax(one_hot[1:], axis=1)\n",
        "\n",
        "      output, hidden  = model(input, hidden)\n",
        "\n",
        "      loss = loss_func(output.squeeze(1), target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    # Store average loss for this epoch\n",
        "    avg_loss = total_loss / n_five_words\n",
        "    rnn_losses.append(avg_loss)\n",
        "\n",
        "    if i%10 == 0:\n",
        "      print('epoch%d'%i)\n",
        "      print(f'Loss: {avg_loss:.4f}')\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "  torch.save(model.state_dict(), 'trained_rnn.pth')\n",
        "  model.load_state_dict(torch.load('trained_rnn.pth'))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    total = 0\n",
        "    positive = 0\n",
        "    total_text = 0\n",
        "    positive_text = 0\n",
        "    word_acc_list = []\n",
        "\n",
        "    for i in range(n_five_words):\n",
        "      string = five_words[i]\n",
        "      one_hot = torch.from_numpy(word_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "      hidden = model.init_hidden()\n",
        "      input = one_hot[0:-1]\n",
        "      input = torch.unsqueeze(input, 1)\n",
        "      target = np.argmax(one_hot[1:], axis=1)\n",
        "      output, hidden = model(input, hidden)\n",
        "      output = output.squeeze()\n",
        "\n",
        "      output_string = string[0]\n",
        "      word_correct_chars = 0\n",
        "\n",
        "      for j in range(output.size()[0]):\n",
        "        output_string += onehot_to_word(output[j].data)\n",
        "        total_text += 1\n",
        "\n",
        "        if string[j+1] == output_string[-1]:\n",
        "          positive_text += 1\n",
        "          word_correct_chars += 1\n",
        "\n",
        "      # Calculate accuracy for this word\n",
        "      word_accuracy = word_correct_chars / len(string[1:])\n",
        "      word_acc_list.append(word_accuracy)\n",
        "\n",
        "      total += 1\n",
        "      if string[-1] == output_string[-1]:\n",
        "        positive += 1\n",
        "\n",
        "      print('%d GT:%s OUT:%s ACC:%.3f'%(i+1, string, output_string, word_accuracy))\n",
        "\n",
        "    # Store word accuracies\n",
        "    rnn_word_accuracies = word_acc_list\n",
        "\n",
        "    print('final text accuracy %d/%d (%.4f)'%(positive, total, positive/total))\n",
        "    print('whole text accuracy %d/%d (%.4f)' % (positive_text, total_text, positive_text / total_text))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "\n",
        "### Improvement using LSTM\n",
        "\n",
        "The improved LSTM model addresses the limitations of the vanilla RNN by incorporating Long Short-Term Memory units, which are specifically designed to handle long-term dependencies through gating mechanisms. The architecture is significantly enhanced with three LSTM layers instead of one, and the hidden size is increased from 26 to 64 units, providing greater model capacity. A dropout rate of 0.2 is applied between LSTM layers to prevent overfitting, and an additional fully connected layer maps the LSTM output back to the vocabulary size. The training process is extended to 1500 epochs with gradient clipping to prevent gradient explosion, weight decay for regularization, and a modified learning rate scheduler that reduces the rate by half every 500 epochs. And using CUDA, it will do more fast than before. The LSTM's cell state and hidden state allow it to selectively remember and forget information over longer sequences, making it more effective at capturing character-level patterns and dependencies. This results in improved accuracy for both individual character predictions and complete word reconstruction compared to the vanilla RNN approach.\n"
      ],
      "metadata": {
        "id": "k5z4XAQYQisN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "chars = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "char_list = [i for i in chars]\n",
        "n_letters = len(char_list)\n",
        "\n",
        "n_layers = 3\n",
        "\n",
        "five_words = ['basic','beach','below','black','brown','carry','cream','drink','error','event',\n",
        "              'exist','first','funny','guess','human','image','large','magic','mouse','night',\n",
        "              'noise','ocean','often','order','peace','phone','print','quiet','reach','rough',\n",
        "              'round','scene','score','sense','skill','sleep','small','storm','table','think',\n",
        "              'touch','twice','until','upset','voice','waste','watch','white','woman','young']\n",
        "n_five_words = len(five_words)\n",
        "\n",
        "sequence_length = 4\n",
        "\n",
        "def word_to_onehot(string):\n",
        "    one_hot = np.array([]).reshape(0,n_letters)\n",
        "    for i in string:\n",
        "        idx = char_list.index(i)\n",
        "        zero = np.zeros(shape=n_letters, dtype=int)\n",
        "        zero[idx] = 1\n",
        "        one_hot = np.vstack([one_hot, zero])\n",
        "    return one_hot\n",
        "\n",
        "def onehot_to_word(onehot_1):\n",
        "    onehot = torch.Tensor.numpy(onehot_1)\n",
        "    return char_list[onehot.argmax()]\n",
        "\n",
        "class myLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layer, dropout=0.2):\n",
        "        super(myLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layer = num_layer\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layer,\n",
        "            dropout=dropout if num_layer > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        h0 = torch.zeros(self.num_layer, 1, self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layer, 1, self.hidden_size).to(device)\n",
        "        return (h0, c0)\n",
        "\n",
        "# Lists to store loss and accuracy\n",
        "lstm_losses = []\n",
        "lstm_word_accuracies = []\n",
        "\n",
        "def main():\n",
        "    n_hidden = 64\n",
        "    lr = 0.001\n",
        "    epochs = 1500\n",
        "\n",
        "    model = myLSTM(n_letters, n_hidden, n_layers).to(device)\n",
        "\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
        "\n",
        "    print(\"Training started...\")\n",
        "\n",
        "    for i in range(epochs):\n",
        "        total_loss = 0\n",
        "        for j in range(n_five_words):\n",
        "            string = five_words[j]\n",
        "            one_hot = torch.from_numpy(word_to_onehot(string)).type_as(torch.FloatTensor()).to(device)\n",
        "            model.zero_grad()\n",
        "\n",
        "            input = one_hot[0:-1]\n",
        "            input = input.unsqueeze(0)\n",
        "            hidden = model.init_hidden()\n",
        "            target = torch.argmax(one_hot[1:], dim=1).long().to(device)\n",
        "\n",
        "            output, hidden = model(input, hidden)\n",
        "\n",
        "            loss = loss_func(output.squeeze(0), target)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Store average loss for this epoch\n",
        "        avg_loss = total_loss / n_five_words\n",
        "        lstm_losses.append(avg_loss)\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print('Epoch %d, Average Loss: %.4f' % (i, avg_loss))\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    torch.save(model.state_dict(), 'trained_lstm.pth')\n",
        "    model.load_state_dict(torch.load('trained_lstm.pth'))\n",
        "\n",
        "    print(\"\\nEvaluation:\")\n",
        "    with torch.no_grad():\n",
        "        total = 0\n",
        "        positive = 0\n",
        "        total_text = 0\n",
        "        positive_text = 0\n",
        "        word_acc_list = []\n",
        "\n",
        "        for i in range(n_five_words):\n",
        "            string = five_words[i]\n",
        "            one_hot = torch.from_numpy(word_to_onehot(string)).type_as(torch.FloatTensor()).to(device)\n",
        "\n",
        "            input = one_hot[0:-1]\n",
        "            input = input.unsqueeze(0)\n",
        "            hidden = model.init_hidden()\n",
        "            target = torch.argmax(one_hot[1:], dim=1).cpu().numpy()\n",
        "\n",
        "            output, hidden = model(input, hidden)\n",
        "            output = output.squeeze()\n",
        "\n",
        "            output_string = string[0]\n",
        "            word_correct_chars = 0\n",
        "\n",
        "            for j in range(output.size()[0]):\n",
        "                output_string += onehot_to_word(output[j].cpu().data)\n",
        "                total_text += 1\n",
        "\n",
        "                if string[j+1] == output_string[-1]:\n",
        "                    positive_text += 1\n",
        "                    word_correct_chars += 1\n",
        "\n",
        "            # Calculate accuracy for this word\n",
        "            word_accuracy = word_correct_chars / len(string[1:])\n",
        "            word_acc_list.append(word_accuracy)\n",
        "\n",
        "            total += 1\n",
        "            if string[-1] == output_string[-1]:\n",
        "                positive += 1\n",
        "\n",
        "            print('%d GT:%s OUT:%s ACC:%.3f'%(i+1, string, output_string, word_accuracy))\n",
        "\n",
        "        # Store word accuracies\n",
        "        lstm_word_accuracies = word_acc_list\n",
        "\n",
        "        print('final text accuracy %d/%d (%.4f)'%(positive, total, positive/total))\n",
        "        print('whole text accuracy %d/%d (%.4f)' % (positive_text, total_text, positive_text / total_text))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CoM631EM6BZ",
        "outputId": "4c66a71f-ea95-431d-df66-a20cb6cc4439"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training started...\n",
            "Epoch 0, Average Loss: 3.1648\n",
            "Epoch 50, Average Loss: 1.3907\n",
            "Epoch 100, Average Loss: 0.6308\n",
            "Epoch 150, Average Loss: 0.4195\n",
            "Epoch 200, Average Loss: 0.3245\n",
            "Epoch 250, Average Loss: 0.3040\n",
            "Epoch 300, Average Loss: 0.2878\n",
            "Epoch 350, Average Loss: 0.2890\n",
            "Epoch 400, Average Loss: 0.2932\n",
            "Epoch 450, Average Loss: 0.2924\n",
            "Epoch 500, Average Loss: 0.2900\n",
            "Epoch 550, Average Loss: 0.2823\n",
            "Epoch 600, Average Loss: 0.2753\n",
            "Epoch 650, Average Loss: 0.2896\n",
            "Epoch 700, Average Loss: 0.2795\n",
            "Epoch 750, Average Loss: 0.2818\n",
            "Epoch 800, Average Loss: 0.2758\n",
            "Epoch 850, Average Loss: 0.2753\n",
            "Epoch 900, Average Loss: 0.2926\n",
            "Epoch 950, Average Loss: 0.2782\n",
            "Epoch 1000, Average Loss: 0.2649\n",
            "Epoch 1050, Average Loss: 0.2781\n",
            "Epoch 1100, Average Loss: 0.2900\n",
            "Epoch 1150, Average Loss: 0.2789\n",
            "Epoch 1200, Average Loss: 0.2770\n",
            "Epoch 1250, Average Loss: 0.2830\n",
            "Epoch 1300, Average Loss: 0.2639\n",
            "Epoch 1350, Average Loss: 0.2659\n",
            "Epoch 1400, Average Loss: 0.2642\n",
            "Epoch 1450, Average Loss: 0.2737\n",
            "Training completed!\n",
            "\n",
            "Evaluation:\n",
            "1 GT:basic OUT:besic ACC:0.750\n",
            "2 GT:beach OUT:beach ACC:1.000\n",
            "3 GT:below OUT:beaow ACC:0.750\n",
            "4 GT:black OUT:baack ACC:0.750\n",
            "5 GT:brown OUT:beown ACC:0.750\n",
            "6 GT:carry OUT:crrry ACC:0.750\n",
            "7 GT:cream OUT:cream ACC:1.000\n",
            "8 GT:drink OUT:drink ACC:1.000\n",
            "9 GT:error OUT:exror ACC:0.750\n",
            "10 GT:event OUT:exent ACC:0.750\n",
            "11 GT:exist OUT:evist ACC:0.750\n",
            "12 GT:first OUT:first ACC:1.000\n",
            "13 GT:funny OUT:funny ACC:1.000\n",
            "14 GT:guess OUT:guess ACC:1.000\n",
            "15 GT:human OUT:human ACC:1.000\n",
            "16 GT:image OUT:image ACC:1.000\n",
            "17 GT:large OUT:large ACC:1.000\n",
            "18 GT:magic OUT:magic ACC:1.000\n",
            "19 GT:mouse OUT:mause ACC:0.750\n",
            "20 GT:night OUT:night ACC:1.000\n",
            "21 GT:noise OUT:niise ACC:0.750\n",
            "22 GT:ocean OUT:ocean ACC:1.000\n",
            "23 GT:often OUT:orten ACC:0.750\n",
            "24 GT:order OUT:order ACC:1.000\n",
            "25 GT:peace OUT:prace ACC:0.750\n",
            "26 GT:phone OUT:peone ACC:0.750\n",
            "27 GT:print OUT:print ACC:1.000\n",
            "28 GT:quiet OUT:quiet ACC:1.000\n",
            "29 GT:reach OUT:roach ACC:0.750\n",
            "30 GT:rough OUT:rounh ACC:0.750\n",
            "31 GT:round OUT:round ACC:1.000\n",
            "32 GT:scene OUT:seone ACC:0.500\n",
            "33 GT:score OUT:score ACC:1.000\n",
            "34 GT:sense OUT:scnse ACC:0.750\n",
            "35 GT:skill OUT:scill ACC:0.750\n",
            "36 GT:sleep OUT:sceep ACC:0.750\n",
            "37 GT:small OUT:seall ACC:0.750\n",
            "38 GT:storm OUT:scorm ACC:0.750\n",
            "39 GT:table OUT:twble ACC:0.750\n",
            "40 GT:think OUT:twink ACC:0.750\n",
            "41 GT:touch OUT:twuch ACC:0.750\n",
            "42 GT:twice OUT:thice ACC:0.750\n",
            "43 GT:until OUT:uptil ACC:0.750\n",
            "44 GT:upset OUT:upset ACC:1.000\n",
            "45 GT:voice OUT:voice ACC:1.000\n",
            "46 GT:waste OUT:watte ACC:0.750\n",
            "47 GT:watch OUT:wasch ACC:0.750\n",
            "48 GT:white OUT:waite ACC:0.750\n",
            "49 GT:woman OUT:waman ACC:0.750\n",
            "50 GT:young OUT:young ACC:1.000\n",
            "final text accuracy 50/50 (1.0000)\n",
            "whole text accuracy 169/200 (0.8450)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IiFfXo7AVjoy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}